{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, SubsetRandomSampler, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Obtaining dependency information for plotly from https://files.pythonhosted.org/packages/df/79/c80174d711ee26ee5da55a9cc3e248f1ec7a0188b5e4d6bbbbcd09b974b0/plotly-5.17.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached plotly-5.17.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tenacity>=6.2.0 (from plotly)\n",
      "  Obtaining dependency information for tenacity>=6.2.0 from https://files.pythonhosted.org/packages/f4/f1/990741d5bb2487d529d20a433210ffa136a367751e454214013b441c4575/tenacity-8.2.3-py3-none-any.whl.metadata\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\tharsigan j\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from plotly) (23.2)\n",
      "Downloading plotly-5.17.0-py2.py3-none-any.whl (15.6 MB)\n",
      "   ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.6 MB 330.3 kB/s eta 0:00:48\n",
      "   ---------------------------------------- 0.0/15.6 MB 330.3 kB/s eta 0:00:48\n",
      "   ---------------------------------------- 0.0/15.6 MB 178.6 kB/s eta 0:01:28\n",
      "   ---------------------------------------- 0.1/15.6 MB 280.5 kB/s eta 0:00:56\n",
      "   ---------------------------------------- 0.1/15.6 MB 385.0 kB/s eta 0:00:41\n",
      "   ---------------------------------------- 0.1/15.6 MB 385.0 kB/s eta 0:00:41\n",
      "   ---------------------------------------- 0.1/15.6 MB 385.0 kB/s eta 0:00:41\n",
      "   ---------------------------------------- 0.2/15.6 MB 353.1 kB/s eta 0:00:44\n",
      "   ---------------------------------------- 0.2/15.6 MB 353.1 kB/s eta 0:00:44\n",
      "    --------------------------------------- 0.2/15.6 MB 389.1 kB/s eta 0:00:40\n",
      "    --------------------------------------- 0.2/15.6 MB 436.6 kB/s eta 0:00:36\n",
      "    --------------------------------------- 0.3/15.6 MB 448.2 kB/s eta 0:00:35\n",
      "    --------------------------------------- 0.3/15.6 MB 442.4 kB/s eta 0:00:35\n",
      "    --------------------------------------- 0.3/15.6 MB 432.3 kB/s eta 0:00:36\n",
      "    --------------------------------------- 0.4/15.6 MB 474.3 kB/s eta 0:00:33\n",
      "   - -------------------------------------- 0.4/15.6 MB 501.1 kB/s eta 0:00:31\n",
      "   - -------------------------------------- 0.4/15.6 MB 501.1 kB/s eta 0:00:31\n",
      "   - -------------------------------------- 0.5/15.6 MB 505.7 kB/s eta 0:00:31\n",
      "   - -------------------------------------- 0.5/15.6 MB 513.6 kB/s eta 0:00:30\n",
      "   - -------------------------------------- 0.5/15.6 MB 532.5 kB/s eta 0:00:29\n",
      "   - -------------------------------------- 0.6/15.6 MB 568.9 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 0.6/15.6 MB 569.9 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 0.7/15.6 MB 619.2 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 0.8/15.6 MB 655.6 kB/s eta 0:00:23\n",
      "   -- ------------------------------------- 0.8/15.6 MB 663.9 kB/s eta 0:00:23\n",
      "   -- ------------------------------------- 0.8/15.6 MB 663.3 kB/s eta 0:00:23\n",
      "   -- ------------------------------------- 0.9/15.6 MB 663.2 kB/s eta 0:00:23\n",
      "   -- ------------------------------------- 0.9/15.6 MB 647.9 kB/s eta 0:00:23\n",
      "   -- ------------------------------------- 0.9/15.6 MB 640.9 kB/s eta 0:00:23\n",
      "   -- ------------------------------------- 1.0/15.6 MB 669.5 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 1.0/15.6 MB 675.5 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 1.0/15.6 MB 675.5 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 1.1/15.6 MB 674.6 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 1.2/15.6 MB 692.6 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.2/15.6 MB 692.0 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.2/15.6 MB 691.0 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.2/15.6 MB 683.8 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 1.3/15.6 MB 694.6 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.3/15.6 MB 694.6 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.3/15.6 MB 660.8 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 1.3/15.6 MB 660.8 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 1.3/15.6 MB 660.8 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 1.3/15.6 MB 660.8 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/15.6 MB 612.0 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.6/15.6 MB 539.7 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.6/15.6 MB 537.5 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.6/15.6 MB 537.5 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.6/15.6 MB 527.6 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.6/15.6 MB 527.6 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.6/15.6 MB 527.6 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.7/15.6 MB 515.3 kB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 1.7/15.6 MB 513.6 kB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 1.7/15.6 MB 512.5 kB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 1.8/15.6 MB 521.3 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.8/15.6 MB 521.3 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.8/15.6 MB 521.3 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.8/15.6 MB 514.9 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.9/15.6 MB 524.9 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.9/15.6 MB 524.3 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.9/15.6 MB 524.3 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.9/15.6 MB 524.9 kB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 1.9/15.6 MB 524.9 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 2.0/15.6 MB 525.4 kB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 2.0/15.6 MB 525.4 kB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 2.0/15.6 MB 519.5 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 2.0/15.6 MB 518.4 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 2.0/15.6 MB 513.4 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 2.1/15.6 MB 515.1 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 2.1/15.6 MB 524.4 kB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 2.2/15.6 MB 538.3 kB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 2.3/15.6 MB 542.1 kB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 2.3/15.6 MB 542.1 kB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 2.3/15.6 MB 542.1 kB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 2.3/15.6 MB 543.0 kB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 2.3/15.6 MB 543.0 kB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 2.3/15.6 MB 543.0 kB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 2.3/15.6 MB 543.0 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.4/15.6 MB 527.0 kB/s eta 0:00:26\n",
      "   ------ --------------------------------- 2.4/15.6 MB 533.4 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.5/15.6 MB 536.8 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.5/15.6 MB 533.6 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.5/15.6 MB 533.6 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.5/15.6 MB 529.5 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.5/15.6 MB 534.6 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.5/15.6 MB 534.6 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.6/15.6 MB 524.7 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.6/15.6 MB 525.5 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.6/15.6 MB 521.0 kB/s eta 0:00:26\n",
      "   ------ --------------------------------- 2.6/15.6 MB 524.3 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.7/15.6 MB 527.2 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.7/15.6 MB 526.0 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.7/15.6 MB 526.0 kB/s eta 0:00:25\n",
      "   ------ --------------------------------- 2.7/15.6 MB 522.7 kB/s eta 0:00:25\n",
      "   ------- -------------------------------- 2.7/15.6 MB 522.4 kB/s eta 0:00:25\n",
      "   ------- -------------------------------- 2.8/15.6 MB 527.4 kB/s eta 0:00:25\n",
      "   ------- -------------------------------- 2.8/15.6 MB 527.8 kB/s eta 0:00:25\n",
      "   ------- -------------------------------- 2.8/15.6 MB 525.8 kB/s eta 0:00:25\n",
      "   ------- -------------------------------- 2.9/15.6 MB 530.4 kB/s eta 0:00:25\n",
      "   ------- -------------------------------- 2.9/15.6 MB 526.1 kB/s eta 0:00:25\n",
      "   ------- -------------------------------- 3.0/15.6 MB 534.6 kB/s eta 0:00:24\n",
      "   ------- -------------------------------- 3.0/15.6 MB 539.0 kB/s eta 0:00:24\n",
      "   ------- -------------------------------- 3.0/15.6 MB 538.2 kB/s eta 0:00:24\n",
      "   ------- -------------------------------- 3.1/15.6 MB 537.7 kB/s eta 0:00:24\n",
      "   ------- -------------------------------- 3.1/15.6 MB 544.0 kB/s eta 0:00:24\n",
      "   -------- ------------------------------- 3.2/15.6 MB 546.7 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 3.2/15.6 MB 546.7 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 3.2/15.6 MB 546.7 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 3.2/15.6 MB 537.1 kB/s eta 0:00:24\n",
      "   -------- ------------------------------- 3.2/15.6 MB 537.1 kB/s eta 0:00:24\n",
      "   -------- ------------------------------- 3.2/15.6 MB 535.2 kB/s eta 0:00:24\n",
      "   -------- ------------------------------- 3.3/15.6 MB 538.2 kB/s eta 0:00:24\n",
      "   -------- ------------------------------- 3.3/15.6 MB 544.2 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 3.3/15.6 MB 543.3 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 3.4/15.6 MB 543.9 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 3.4/15.6 MB 543.9 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 3.4/15.6 MB 536.0 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 3.4/15.6 MB 541.5 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 3.5/15.6 MB 544.3 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 3.5/15.6 MB 546.4 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 3.5/15.6 MB 545.6 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 3.6/15.6 MB 546.7 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 3.6/15.6 MB 549.0 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 3.6/15.6 MB 549.0 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 3.6/15.6 MB 544.1 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 3.6/15.6 MB 543.6 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 3.7/15.6 MB 545.9 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 3.7/15.6 MB 545.9 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 3.7/15.6 MB 547.4 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 3.8/15.6 MB 551.1 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 3.8/15.6 MB 553.1 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 3.8/15.6 MB 553.1 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 3.9/15.6 MB 548.6 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 3.9/15.6 MB 548.6 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 3.9/15.6 MB 545.9 kB/s eta 0:00:22\n",
      "   ---------- ----------------------------- 3.9/15.6 MB 548.1 kB/s eta 0:00:22\n",
      "   ---------- ----------------------------- 4.0/15.6 MB 553.0 kB/s eta 0:00:22\n",
      "   ---------- ----------------------------- 4.0/15.6 MB 556.5 kB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 4.0/15.6 MB 556.5 kB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 4.0/15.6 MB 556.5 kB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 4.1/15.6 MB 552.8 kB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 4.1/15.6 MB 552.8 kB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 4.2/15.6 MB 559.8 kB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 4.2/15.6 MB 560.2 kB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 4.2/15.6 MB 559.3 kB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 4.3/15.6 MB 561.2 kB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 4.3/15.6 MB 560.2 kB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 4.3/15.6 MB 560.2 kB/s eta 0:00:21\n",
      "   ----------- ---------------------------- 4.4/15.6 MB 565.7 kB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 4.4/15.6 MB 567.6 kB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 4.5/15.6 MB 569.4 kB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 4.5/15.6 MB 569.4 kB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 4.5/15.6 MB 565.2 kB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 4.6/15.6 MB 572.2 kB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 4.6/15.6 MB 572.5 kB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 4.6/15.6 MB 574.2 kB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 4.6/15.6 MB 574.2 kB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 4.6/15.6 MB 574.2 kB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 4.7/15.6 MB 565.8 kB/s eta 0:00:20\n",
      "   ------------ --------------------------- 4.7/15.6 MB 568.8 kB/s eta 0:00:20\n",
      "   ------------ --------------------------- 4.8/15.6 MB 571.8 kB/s eta 0:00:20\n",
      "   ------------ --------------------------- 4.8/15.6 MB 573.4 kB/s eta 0:00:19\n",
      "   ------------ --------------------------- 4.9/15.6 MB 576.3 kB/s eta 0:00:19\n",
      "   ------------ --------------------------- 4.9/15.6 MB 578.9 kB/s eta 0:00:19\n",
      "   ------------ --------------------------- 4.9/15.6 MB 578.9 kB/s eta 0:00:19\n",
      "   ------------ --------------------------- 4.9/15.6 MB 572.9 kB/s eta 0:00:19\n",
      "   ------------ --------------------------- 5.0/15.6 MB 575.7 kB/s eta 0:00:19\n",
      "   ------------ --------------------------- 5.0/15.6 MB 582.0 kB/s eta 0:00:19\n",
      "   ------------- -------------------------- 5.1/15.6 MB 588.3 kB/s eta 0:00:18\n",
      "   ------------- -------------------------- 5.2/15.6 MB 591.0 kB/s eta 0:00:18\n",
      "   ------------- -------------------------- 5.2/15.6 MB 591.3 kB/s eta 0:00:18\n",
      "   ------------- -------------------------- 5.2/15.6 MB 592.7 kB/s eta 0:00:18\n",
      "   ------------- -------------------------- 5.3/15.6 MB 590.7 kB/s eta 0:00:18\n",
      "   ------------- -------------------------- 5.3/15.6 MB 591.0 kB/s eta 0:00:18\n",
      "   ------------- -------------------------- 5.3/15.6 MB 590.3 kB/s eta 0:00:18\n",
      "   ------------- -------------------------- 5.3/15.6 MB 590.6 kB/s eta 0:00:18\n",
      "   ------------- -------------------------- 5.4/15.6 MB 596.6 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.5/15.6 MB 601.4 kB/s eta 0:00:17\n",
      "   -------------- ------------------------- 5.5/15.6 MB 601.4 kB/s eta 0:00:17\n",
      "   -------------- ------------------------- 5.5/15.6 MB 601.4 kB/s eta 0:00:17\n",
      "   -------------- ------------------------- 5.5/15.6 MB 601.4 kB/s eta 0:00:17\n",
      "   -------------- ------------------------- 5.5/15.6 MB 591.4 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.5/15.6 MB 591.4 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.5/15.6 MB 591.4 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.5/15.6 MB 591.4 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.5/15.6 MB 591.4 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.6/15.6 MB 582.4 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.6/15.6 MB 582.4 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.6/15.6 MB 580.9 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.6/15.6 MB 579.3 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.6/15.6 MB 577.5 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.6/15.6 MB 577.5 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.7/15.6 MB 576.1 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.7/15.6 MB 574.5 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.7/15.6 MB 573.8 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.7/15.6 MB 573.8 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.7/15.6 MB 573.8 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.7/15.6 MB 567.7 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.7/15.6 MB 566.2 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.7/15.6 MB 563.7 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.8/15.6 MB 566.2 kB/s eta 0:00:18\n",
      "   -------------- ------------------------- 5.8/15.6 MB 568.6 kB/s eta 0:00:18\n",
      "   --------------- ------------------------ 5.9/15.6 MB 569.0 kB/s eta 0:00:18\n",
      "   --------------- ------------------------ 5.9/15.6 MB 573.3 kB/s eta 0:00:17\n",
      "   --------------- ------------------------ 6.0/15.6 MB 574.7 kB/s eta 0:00:17\n",
      "   --------------- ------------------------ 6.0/15.6 MB 577.0 kB/s eta 0:00:17\n",
      "   --------------- ------------------------ 6.1/15.6 MB 578.1 kB/s eta 0:00:17\n",
      "   --------------- ------------------------ 6.1/15.6 MB 576.7 kB/s eta 0:00:17\n",
      "   --------------- ------------------------ 6.1/15.6 MB 577.0 kB/s eta 0:00:17\n",
      "   --------------- ------------------------ 6.2/15.6 MB 578.4 kB/s eta 0:00:17\n",
      "   --------------- ------------------------ 6.2/15.6 MB 579.7 kB/s eta 0:00:17\n",
      "   --------------- ------------------------ 6.2/15.6 MB 580.0 kB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 6.3/15.6 MB 587.0 kB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 6.4/15.6 MB 590.0 kB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 6.4/15.6 MB 589.4 kB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 6.5/15.6 MB 591.6 kB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 6.6/15.6 MB 596.5 kB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 6.6/15.6 MB 595.8 kB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 6.6/15.6 MB 595.1 kB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 6.6/15.6 MB 595.5 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 6.7/15.6 MB 596.7 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 6.7/15.6 MB 596.0 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 6.7/15.6 MB 596.0 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 6.7/15.6 MB 592.9 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 6.7/15.6 MB 592.2 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 6.7/15.6 MB 591.5 kB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 6.8/15.6 MB 592.8 kB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 6.8/15.6 MB 592.8 kB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 6.9/15.6 MB 593.3 kB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 6.9/15.6 MB 594.4 kB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 6.9/15.6 MB 594.4 kB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 6.9/15.6 MB 592.2 kB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 7.0/15.6 MB 591.7 kB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 7.0/15.6 MB 596.3 kB/s eta 0:00:15\n",
      "   ------------------ --------------------- 7.1/15.6 MB 600.8 kB/s eta 0:00:15\n",
      "   ------------------ --------------------- 7.1/15.6 MB 600.8 kB/s eta 0:00:15\n",
      "   ------------------ --------------------- 7.2/15.6 MB 601.2 kB/s eta 0:00:15\n",
      "   ------------------ --------------------- 7.2/15.6 MB 599.0 kB/s eta 0:00:15\n",
      "   ------------------ --------------------- 7.3/15.6 MB 602.7 kB/s eta 0:00:14\n",
      "   ------------------ --------------------- 7.3/15.6 MB 605.3 kB/s eta 0:00:14\n",
      "   ------------------ --------------------- 7.4/15.6 MB 605.5 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 7.4/15.6 MB 609.9 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 7.5/15.6 MB 611.7 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 7.5/15.6 MB 610.2 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 7.5/15.6 MB 610.4 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 7.5/15.6 MB 610.4 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 7.5/15.6 MB 610.4 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 7.6/15.6 MB 606.8 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 7.6/15.6 MB 611.1 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 7.7/15.6 MB 614.5 kB/s eta 0:00:13\n",
      "   ------------------- -------------------- 7.8/15.6 MB 616.3 kB/s eta 0:00:13\n",
      "   -------------------- ------------------- 7.8/15.6 MB 618.1 kB/s eta 0:00:13\n",
      "   -------------------- ------------------- 7.9/15.6 MB 619.9 kB/s eta 0:00:13\n",
      "   -------------------- ------------------- 8.0/15.6 MB 624.0 kB/s eta 0:00:13\n",
      "   -------------------- ------------------- 8.0/15.6 MB 624.1 kB/s eta 0:00:13\n",
      "   -------------------- ------------------- 8.0/15.6 MB 626.6 kB/s eta 0:00:13\n",
      "   -------------------- ------------------- 8.1/15.6 MB 626.7 kB/s eta 0:00:13\n",
      "   -------------------- ------------------- 8.1/15.6 MB 628.4 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 8.2/15.6 MB 632.4 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 8.2/15.6 MB 632.5 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 8.4/15.6 MB 640.5 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 8.4/15.6 MB 642.9 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 8.5/15.6 MB 646.8 kB/s eta 0:00:11\n",
      "   --------------------- ------------------ 8.6/15.6 MB 649.9 kB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 8.6/15.6 MB 650.7 kB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 8.7/15.6 MB 654.6 kB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 8.8/15.6 MB 656.1 kB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 8.9/15.6 MB 661.5 kB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 9.0/15.6 MB 666.0 kB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 9.1/15.6 MB 671.3 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 9.1/15.6 MB 673.5 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 9.2/15.6 MB 676.4 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 9.3/15.6 MB 680.9 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 9.4/15.6 MB 683.1 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 9.5/15.6 MB 688.2 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.5/15.6 MB 691.1 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.5/15.6 MB 691.1 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.6/15.6 MB 689.4 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.6/15.6 MB 689.4 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.6/15.6 MB 689.4 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.6/15.6 MB 689.4 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.6/15.6 MB 689.4 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.6/15.6 MB 689.4 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.6/15.6 MB 689.4 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.6/15.6 MB 689.4 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.6/15.6 MB 671.9 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 9.6/15.6 MB 671.9 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 9.6/15.6 MB 669.6 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.6/15.6 MB 671.1 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.7/15.6 MB 670.3 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.7/15.6 MB 671.0 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.7/15.6 MB 671.0 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.7/15.6 MB 671.0 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.7/15.6 MB 665.9 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.7/15.6 MB 665.9 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.7/15.6 MB 665.9 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 9.7/15.6 MB 665.9 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 9.8/15.6 MB 660.2 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 9.9/15.6 MB 662.3 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 9.9/15.6 MB 662.3 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 9.9/15.6 MB 661.5 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 9.9/15.6 MB 660.8 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 9.9/15.6 MB 660.8 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 9.9/15.6 MB 660.8 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 9.9/15.6 MB 660.8 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 9.9/15.6 MB 660.8 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 9.9/15.6 MB 660.8 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 10.2/15.6 MB 664.1 kB/s eta 0:00:09\n",
      "   -------------------------- ------------- 10.3/15.6 MB 677.8 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 10.3/15.6 MB 677.1 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 10.4/15.6 MB 681.3 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 10.4/15.6 MB 679.9 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 10.4/15.6 MB 679.9 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 10.4/15.6 MB 678.5 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 10.5/15.6 MB 680.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 10.5/15.6 MB 680.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 10.5/15.6 MB 678.5 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 10.5/15.6 MB 679.9 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.6/15.6 MB 679.9 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.6/15.6 MB 679.9 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.6/15.6 MB 677.8 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.6/15.6 MB 677.8 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.6/15.6 MB 677.8 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.6/15.6 MB 675.6 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.7/15.6 MB 673.6 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.7/15.6 MB 673.6 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.7/15.6 MB 673.6 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.7/15.6 MB 673.6 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.7/15.6 MB 667.3 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.8/15.6 MB 668.8 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.8/15.6 MB 668.7 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.8/15.6 MB 666.7 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.8/15.6 MB 666.7 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 10.9/15.6 MB 666.0 kB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 11.1/15.6 MB 668.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 11.1/15.6 MB 669.4 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 11.1/15.6 MB 670.1 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 11.2/15.6 MB 670.8 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 11.2/15.6 MB 669.4 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 11.2/15.6 MB 668.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 11.3/15.6 MB 671.5 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 11.4/15.6 MB 672.2 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 11.5/15.6 MB 677.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 11.5/15.6 MB 677.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 11.6/15.6 MB 726.6 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 11.7/15.6 MB 722.6 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 11.7/15.6 MB 720.3 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 11.7/15.6 MB 719.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 11.8/15.6 MB 717.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 11.8/15.6 MB 714.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 11.8/15.6 MB 714.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 11.8/15.6 MB 708.6 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 11.8/15.6 MB 712.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 11.9/15.6 MB 718.7 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 11.9/15.6 MB 718.7 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 11.9/15.6 MB 715.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 11.9/15.6 MB 717.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 12.0/15.6 MB 717.9 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 12.0/15.6 MB 717.9 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 12.0/15.6 MB 711.6 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 12.0/15.6 MB 716.3 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 12.1/15.6 MB 717.9 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 12.1/15.6 MB 721.8 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 12.2/15.6 MB 719.4 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 12.3/15.6 MB 726.6 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 12.3/15.6 MB 731.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 12.3/15.6 MB 729.1 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 12.3/15.6 MB 728.3 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 12.4/15.6 MB 725.8 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 12.4/15.6 MB 724.2 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 12.5/15.6 MB 722.6 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 12.5/15.6 MB 723.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 12.5/15.6 MB 723.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 12.5/15.6 MB 717.9 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 12.5/15.6 MB 721.1 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 12.6/15.6 MB 731.5 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 12.7/15.6 MB 730.7 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 12.7/15.6 MB 729.9 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 12.7/15.6 MB 735.6 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 12.9/15.6 MB 747.4 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.0/15.6 MB 758.7 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.1/15.6 MB 764.0 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.1/15.6 MB 764.0 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.1/15.6 MB 759.5 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.1/15.6 MB 759.5 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.1/15.6 MB 758.7 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.1/15.6 MB 758.7 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.1/15.6 MB 758.7 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.1/15.6 MB 758.7 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.2/15.6 MB 747.4 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.2/15.6 MB 748.2 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.2/15.6 MB 747.4 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 13.3/15.6 MB 745.7 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 13.3/15.6 MB 748.2 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 13.4/15.6 MB 760.4 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 13.5/15.6 MB 764.8 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 13.6/15.6 MB 774.8 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 13.7/15.6 MB 773.9 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 13.7/15.6 MB 774.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 13.7/15.6 MB 774.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 13.8/15.6 MB 773.9 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 13.9/15.6 MB 783.2 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 13.9/15.6 MB 784.1 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 14.0/15.6 MB 786.9 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 14.0/15.6 MB 788.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 14.1/15.6 MB 791.7 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.2/15.6 MB 800.3 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.3/15.6 MB 809.3 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.4/15.6 MB 815.3 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 14.5/15.6 MB 817.3 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 14.6/15.6 MB 823.6 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 14.6/15.6 MB 824.6 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 14.7/15.6 MB 824.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 14.8/15.6 MB 830.8 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 14.8/15.6 MB 830.8 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 14.8/15.6 MB 824.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.9/15.6 MB 827.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.9/15.6 MB 835.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.0/15.6 MB 835.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.0/15.6 MB 836.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.0/15.6 MB 836.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.0/15.6 MB 827.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.0/15.6 MB 824.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.1/15.6 MB 826.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.1/15.6 MB 824.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.1/15.6 MB 822.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.6 MB 830.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.3/15.6 MB 831.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.3/15.6 MB 830.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.3/15.6 MB 830.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.3/15.6 MB 830.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.3/15.6 MB 819.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.4/15.6 MB 820.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.4/15.6 MB 819.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.4/15.6 MB 818.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.6 MB 816.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.6 MB 818.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.6 MB 818.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 822.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 822.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.6/15.6 MB 815.9 kB/s eta 0:00:00\n",
      "Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.17.0 tenacity-8.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using\", device, \"device.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.ImageOps\n",
    "\n",
    "def to_negative(img):\n",
    "    img = PIL.ImageOps.invert(img)\n",
    "    return img\n",
    "class Negative(object):\n",
    "    \"\"\"Convert image to negative.\n",
    "\n",
    "    Args:\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        PIL Image: Negative version of the input.\n",
    "         \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be converted to Negative.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Negatived image.\n",
    "        \"\"\"\n",
    "        return to_negative(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_H = 200\n",
    "IMG_W = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "# Define the transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_H, IMG_W)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.ImageFolder(\n",
    "    root=r'C:\\Users\\Tharsigan J\\Desktop\\cnn\\root',\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 746\n",
       "    Root location: C:\\Users\\Tharsigan J\\Desktop\\cnn\\root\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(200, 200), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 746, divide our dataset into validation set also 746/5 -> 149.2\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset=dataset, \n",
    "    lengths = [596, 150],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x2122da5d250>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformation = transforms.Compose([\n",
    "    transforms.RandomRotation(5),\n",
    "    Negative(),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        scale=(1.1, 1.1), \n",
    "        shear=0.9\n",
    "    ),\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x2122da6b100>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformation \n",
    "augments = copy.deepcopy(train_dataset) \n",
    "augments.dataset.transform = train_transformation \n",
    "augments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_dataset =  torch.utils.data.ConcatDataset([train_dataset,augments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model1, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "              \n",
    "              nn.Conv2d(in_channels=3, out_channels=128, kernel_size=5,padding=1),  \n",
    "              nn.BatchNorm2d(128),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2), \n",
    "\n",
    "              nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=0), \n",
    "              nn.BatchNorm2d(256),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2), \n",
    "\n",
    "              nn.Conv2d(in_channels=256, out_channels=512, kernel_size=5,padding=0),\n",
    "              nn.BatchNorm2d(512),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            \n",
    "              nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3,padding=0), \n",
    "              nn.BatchNorm2d(1024),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2,),  \n",
    "\n",
    "              nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=5,padding=1),\n",
    "              nn.BatchNorm2d(2048),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            \n",
    "            \n",
    "              nn.Flatten() ,\n",
    "              nn.Dropout(0.5),  \n",
    "            \n",
    "              nn.Linear(in_features= 32768, out_features=1024),\n",
    "              nn.ReLU(),\n",
    "              nn.Dropout(0.3),\n",
    "              nn.Linear(in_features=1024, out_features=256),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(in_features=256, out_features=64),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(in_features=64, out_features=1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "             return  self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1       [200, 128, 198, 198]           9,728\n",
      "       BatchNorm2d-2       [200, 128, 198, 198]             256\n",
      "              ReLU-3       [200, 128, 198, 198]               0\n",
      "         MaxPool2d-4         [200, 128, 99, 99]               0\n",
      "            Conv2d-5         [200, 256, 97, 97]         295,168\n",
      "       BatchNorm2d-6         [200, 256, 97, 97]             512\n",
      "              ReLU-7         [200, 256, 97, 97]               0\n",
      "         MaxPool2d-8         [200, 256, 48, 48]               0\n",
      "            Conv2d-9         [200, 512, 44, 44]       3,277,312\n",
      "      BatchNorm2d-10         [200, 512, 44, 44]           1,024\n",
      "             ReLU-11         [200, 512, 44, 44]               0\n",
      "        MaxPool2d-12         [200, 512, 22, 22]               0\n",
      "           Conv2d-13        [200, 1024, 20, 20]       4,719,616\n",
      "      BatchNorm2d-14        [200, 1024, 20, 20]           2,048\n",
      "             ReLU-15        [200, 1024, 20, 20]               0\n",
      "        MaxPool2d-16        [200, 1024, 10, 10]               0\n",
      "           Conv2d-17          [200, 2048, 8, 8]      52,430,848\n",
      "      BatchNorm2d-18          [200, 2048, 8, 8]           4,096\n",
      "             ReLU-19          [200, 2048, 8, 8]               0\n",
      "        MaxPool2d-20          [200, 2048, 4, 4]               0\n",
      "          Flatten-21               [200, 32768]               0\n",
      "          Dropout-22               [200, 32768]               0\n",
      "           Linear-23                [200, 1024]      33,555,456\n",
      "             ReLU-24                [200, 1024]               0\n",
      "          Dropout-25                [200, 1024]               0\n",
      "           Linear-26                 [200, 256]         262,400\n",
      "             ReLU-27                 [200, 256]               0\n",
      "           Linear-28                  [200, 64]          16,448\n",
      "             ReLU-29                  [200, 64]               0\n",
      "           Linear-30                   [200, 1]              65\n",
      "================================================================\n",
      "Total params: 94,574,977\n",
      "Trainable params: 94,574,977\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 91.55\n",
      "Forward/backward pass size (MB): 11746.06\n",
      "Params size (MB): 360.77\n",
      "Estimated Total Size (MB): 12198.39\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tharsigan j\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchsummary\\torchsummary.py:93: RuntimeWarning: overflow encountered in scalar add\n",
      "  total_output += np.prod(summary[layer][\"output_shape\"])\n"
     ]
    }
   ],
   "source": [
    "cnn_1 = Model1()\n",
    "cnn_1 = cnn_1.to(device)\n",
    "summary(cnn_1,(3,200,200),200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1       [200, 128, 198, 198]           9,728\n",
      "       BatchNorm2d-2       [200, 128, 198, 198]             256\n",
      "              ReLU-3       [200, 128, 198, 198]               0\n",
      "         MaxPool2d-4         [200, 128, 99, 99]               0\n",
      "            Conv2d-5         [200, 256, 97, 97]         295,168\n",
      "       BatchNorm2d-6         [200, 256, 97, 97]             512\n",
      "              ReLU-7         [200, 256, 97, 97]               0\n",
      "         MaxPool2d-8         [200, 256, 48, 48]               0\n",
      "            Conv2d-9         [200, 512, 44, 44]       3,277,312\n",
      "      BatchNorm2d-10         [200, 512, 44, 44]           1,024\n",
      "             ReLU-11         [200, 512, 44, 44]               0\n",
      "        MaxPool2d-12         [200, 512, 22, 22]               0\n",
      "           Conv2d-13        [200, 1024, 20, 20]       4,719,616\n",
      "      BatchNorm2d-14        [200, 1024, 20, 20]           2,048\n",
      "             ReLU-15        [200, 1024, 20, 20]               0\n",
      "        MaxPool2d-16        [200, 1024, 10, 10]               0\n",
      "           Conv2d-17          [200, 2048, 8, 8]      52,430,848\n",
      "      BatchNorm2d-18          [200, 2048, 8, 8]           4,096\n",
      "             ReLU-19          [200, 2048, 8, 8]               0\n",
      "        MaxPool2d-20          [200, 2048, 4, 4]               0\n",
      "          Flatten-21               [200, 32768]               0\n",
      "          Dropout-22               [200, 32768]               0\n",
      "           Linear-23                [200, 1024]      33,555,456\n",
      "             ReLU-24                [200, 1024]               0\n",
      "          Dropout-25                [200, 1024]               0\n",
      "           Linear-26                 [200, 256]         262,400\n",
      "             ReLU-27                 [200, 256]               0\n",
      "           Linear-28                  [200, 64]          16,448\n",
      "             ReLU-29                  [200, 64]               0\n",
      "           Linear-30                   [200, 1]              65\n",
      "================================================================\n",
      "Total params: 94,574,977\n",
      "Trainable params: 94,574,977\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 91.55\n",
      "Forward/backward pass size (MB): 11746.06\n",
      "Params size (MB): 360.77\n",
      "Estimated Total Size (MB): 12198.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(cnn_1,(3,200,200),200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2\n",
    "\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "              \n",
    "              nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3),  \n",
    "              nn.BatchNorm2d(128),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2), \n",
    "\n",
    "              nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3), \n",
    "              nn.BatchNorm2d(256),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2), \n",
    "\n",
    "              nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3),\n",
    "              nn.BatchNorm2d(512),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2),  \n",
    "\n",
    "              nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3), \n",
    "              nn.BatchNorm2d(1024),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "              nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=3),\n",
    "              nn.BatchNorm2d(2048),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "              nn.Flatten() ,\n",
    "              nn.Dropout(0.2),  \n",
    "            \n",
    "                nn.Linear(in_features= 32768, out_features=2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(in_features=2048, out_features=512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=512, out_features=128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=128, out_features=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return  self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [32, 128, 198, 198]           3,584\n",
      "       BatchNorm2d-2        [32, 128, 198, 198]             256\n",
      "              ReLU-3        [32, 128, 198, 198]               0\n",
      "         MaxPool2d-4          [32, 128, 99, 99]               0\n",
      "            Conv2d-5          [32, 256, 97, 97]         295,168\n",
      "       BatchNorm2d-6          [32, 256, 97, 97]             512\n",
      "              ReLU-7          [32, 256, 97, 97]               0\n",
      "         MaxPool2d-8          [32, 256, 48, 48]               0\n",
      "            Conv2d-9          [32, 512, 46, 46]       1,180,160\n",
      "      BatchNorm2d-10          [32, 512, 46, 46]           1,024\n",
      "             ReLU-11          [32, 512, 46, 46]               0\n",
      "        MaxPool2d-12          [32, 512, 23, 23]               0\n",
      "           Conv2d-13         [32, 1024, 21, 21]       4,719,616\n",
      "      BatchNorm2d-14         [32, 1024, 21, 21]           2,048\n",
      "             ReLU-15         [32, 1024, 21, 21]               0\n",
      "        MaxPool2d-16         [32, 1024, 10, 10]               0\n",
      "           Conv2d-17           [32, 2048, 8, 8]      18,876,416\n",
      "      BatchNorm2d-18           [32, 2048, 8, 8]           4,096\n",
      "             ReLU-19           [32, 2048, 8, 8]               0\n",
      "        MaxPool2d-20           [32, 2048, 4, 4]               0\n",
      "          Flatten-21                [32, 32768]               0\n",
      "          Dropout-22                [32, 32768]               0\n",
      "           Linear-23                 [32, 2048]      67,110,912\n",
      "             ReLU-24                 [32, 2048]               0\n",
      "          Dropout-25                 [32, 2048]               0\n",
      "           Linear-26                  [32, 512]       1,049,088\n",
      "             ReLU-27                  [32, 512]               0\n",
      "           Linear-28                  [32, 128]          65,664\n",
      "             ReLU-29                  [32, 128]               0\n",
      "           Linear-30                    [32, 1]             129\n",
      "================================================================\n",
      "Total params: 93,308,673\n",
      "Trainable params: 93,308,673\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 14.65\n",
      "Forward/backward pass size (MB): 7227.03\n",
      "Params size (MB): 355.94\n",
      "Estimated Total Size (MB): 7597.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cnn_2 = Model2()\n",
    "cnn_2 = cnn_2.to(device)\n",
    "summary(cnn_2,(3,200,200),32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model3, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "              \n",
    "              nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5,padding=1),  \n",
    "              nn.BatchNorm2d(64),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2), \n",
    "\n",
    "              nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=0), \n",
    "              nn.BatchNorm2d(128),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2), \n",
    "\n",
    "              nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5,padding=0),\n",
    "              nn.BatchNorm2d(256),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2,stride=2),  \n",
    "\n",
    "              nn.Conv2d(in_channels=256, out_channels=500, kernel_size=3,padding=0), \n",
    "              nn.BatchNorm2d(500),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(kernel_size=2,stride=2),  \n",
    "\n",
    "#               nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=5,padding=1),\n",
    "#               nn.BatchNorm2d(2048),\n",
    "#               nn.ReLU(),\n",
    "#               nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            \n",
    "            \n",
    "              nn.Flatten() ,\n",
    "              nn.Dropout(0.2),  \n",
    "            \n",
    "                nn.Linear(in_features= 50000, out_features=1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(in_features=1024, out_features=256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=256, out_features=64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=64, out_features=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return  self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 198, 198]           4,864\n",
      "       BatchNorm2d-2         [32, 64, 198, 198]             128\n",
      "              ReLU-3         [32, 64, 198, 198]               0\n",
      "         MaxPool2d-4           [32, 64, 99, 99]               0\n",
      "            Conv2d-5          [32, 128, 97, 97]          73,856\n",
      "       BatchNorm2d-6          [32, 128, 97, 97]             256\n",
      "              ReLU-7          [32, 128, 97, 97]               0\n",
      "         MaxPool2d-8          [32, 128, 48, 48]               0\n",
      "            Conv2d-9          [32, 256, 44, 44]         819,456\n",
      "      BatchNorm2d-10          [32, 256, 44, 44]             512\n",
      "             ReLU-11          [32, 256, 44, 44]               0\n",
      "        MaxPool2d-12          [32, 256, 22, 22]               0\n",
      "           Conv2d-13          [32, 500, 20, 20]       1,152,500\n",
      "      BatchNorm2d-14          [32, 500, 20, 20]           1,000\n",
      "             ReLU-15          [32, 500, 20, 20]               0\n",
      "        MaxPool2d-16          [32, 500, 10, 10]               0\n",
      "          Flatten-17                [32, 50000]               0\n",
      "          Dropout-18                [32, 50000]               0\n",
      "           Linear-19                 [32, 1024]      51,201,024\n",
      "             ReLU-20                 [32, 1024]               0\n",
      "          Dropout-21                 [32, 1024]               0\n",
      "           Linear-22                  [32, 256]         262,400\n",
      "             ReLU-23                  [32, 256]               0\n",
      "           Linear-24                   [32, 64]          16,448\n",
      "             ReLU-25                   [32, 64]               0\n",
      "           Linear-26                    [32, 1]              65\n",
      "================================================================\n",
      "Total params: 53,532,509\n",
      "Trainable params: 53,532,509\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 14.65\n",
      "Forward/backward pass size (MB): 3522.18\n",
      "Params size (MB): 204.21\n",
      "Estimated Total Size (MB): 3741.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cnn_3 = Model3()\n",
    "cnn_3 = cnn_3.to(device)\n",
    "summary(cnn_3,(3,200,200),32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tharsigan j\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\users\\tharsigan j\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\Tharsigan J/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "vgg19 = models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in vgg19.parameters():\n",
    "   param.requires_grad = False\n",
    "\n",
    "vgg19.classifier[6].in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_features = vgg_based.classifier[6].in_features\n",
    "# features = list(vgg_based.classifier.children())[:-1] # Remove last layer\n",
    "# features.extend([torch.nn.Linear(number_features, len(class_names))])\n",
    "# vgg_based.classifier = torch.nn.Sequential(*features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19.classifier[6] = nn.Sequential(\n",
    "    nn.Linear(vgg19.classifier[6].in_features,512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 200, 200]           1,792\n",
      "              ReLU-2         [32, 64, 200, 200]               0\n",
      "            Conv2d-3         [32, 64, 200, 200]          36,928\n",
      "              ReLU-4         [32, 64, 200, 200]               0\n",
      "         MaxPool2d-5         [32, 64, 100, 100]               0\n",
      "            Conv2d-6        [32, 128, 100, 100]          73,856\n",
      "              ReLU-7        [32, 128, 100, 100]               0\n",
      "            Conv2d-8        [32, 128, 100, 100]         147,584\n",
      "              ReLU-9        [32, 128, 100, 100]               0\n",
      "        MaxPool2d-10          [32, 128, 50, 50]               0\n",
      "           Conv2d-11          [32, 256, 50, 50]         295,168\n",
      "             ReLU-12          [32, 256, 50, 50]               0\n",
      "           Conv2d-13          [32, 256, 50, 50]         590,080\n",
      "             ReLU-14          [32, 256, 50, 50]               0\n",
      "           Conv2d-15          [32, 256, 50, 50]         590,080\n",
      "             ReLU-16          [32, 256, 50, 50]               0\n",
      "           Conv2d-17          [32, 256, 50, 50]         590,080\n",
      "             ReLU-18          [32, 256, 50, 50]               0\n",
      "        MaxPool2d-19          [32, 256, 25, 25]               0\n",
      "           Conv2d-20          [32, 512, 25, 25]       1,180,160\n",
      "             ReLU-21          [32, 512, 25, 25]               0\n",
      "           Conv2d-22          [32, 512, 25, 25]       2,359,808\n",
      "             ReLU-23          [32, 512, 25, 25]               0\n",
      "           Conv2d-24          [32, 512, 25, 25]       2,359,808\n",
      "             ReLU-25          [32, 512, 25, 25]               0\n",
      "           Conv2d-26          [32, 512, 25, 25]       2,359,808\n",
      "             ReLU-27          [32, 512, 25, 25]               0\n",
      "        MaxPool2d-28          [32, 512, 12, 12]               0\n",
      "           Conv2d-29          [32, 512, 12, 12]       2,359,808\n",
      "             ReLU-30          [32, 512, 12, 12]               0\n",
      "           Conv2d-31          [32, 512, 12, 12]       2,359,808\n",
      "             ReLU-32          [32, 512, 12, 12]               0\n",
      "           Conv2d-33          [32, 512, 12, 12]       2,359,808\n",
      "             ReLU-34          [32, 512, 12, 12]               0\n",
      "           Conv2d-35          [32, 512, 12, 12]       2,359,808\n",
      "             ReLU-36          [32, 512, 12, 12]               0\n",
      "        MaxPool2d-37            [32, 512, 6, 6]               0\n",
      "AdaptiveAvgPool2d-38            [32, 512, 7, 7]               0\n",
      "           Linear-39                 [32, 4096]     102,764,544\n",
      "             ReLU-40                 [32, 4096]               0\n",
      "          Dropout-41                 [32, 4096]               0\n",
      "           Linear-42                 [32, 4096]      16,781,312\n",
      "             ReLU-43                 [32, 4096]               0\n",
      "          Dropout-44                 [32, 4096]               0\n",
      "           Linear-45                  [32, 512]       2,097,664\n",
      "             ReLU-46                  [32, 512]               0\n",
      "           Linear-47                    [32, 1]             513\n",
      "================================================================\n",
      "Total params: 141,668,417\n",
      "Trainable params: 2,098,177\n",
      "Non-trainable params: 139,570,240\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 14.65\n",
      "Forward/backward pass size (MB): 6077.31\n",
      "Params size (MB): 540.42\n",
      "Estimated Total Size (MB): 6632.38\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vgg19=vgg19.to(device)\n",
    "summary(vgg19,(3,200,200),32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(cnn_1.parameters(), lr=0.001, weight_decay=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(conf_matrix):\n",
    "    tp = conf_matrix[1,1]\n",
    "    fp = conf_matrix[0,1]\n",
    "    tn = conf_matrix[0,0]\n",
    "    fn = conf_matrix[1,0]\n",
    "    print(\"tp:{} | fp:{} | tn:{} | fn:{}\".format(tp,fp,tn,fn))\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
    "    f1 = 2*precision*recall/(precision + recall)\n",
    "    return (precision, recall, f1, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, images_train, images_test, device, lr=0.0001, epochs=30, batch_size=32, l2=0.00001, gamma=0.5, patience=7, folds=5\n",
    "\n",
    "def train_model(params):\n",
    "    model  = params[\"model\"]\n",
    "    images_train  = params[\"train\"]\n",
    "    images_test   = params[\"test\"]\n",
    "    device = params[\"device\"] \n",
    "    lr     = params[\"lr\"]\n",
    "    batch_size  = params[\"batch_size\"]\n",
    "    epochs = params[\"epochs\"]\n",
    "    gamma  = params[\"gamma\"]\n",
    "    patience = params[\"patience\"]\n",
    "    folds = params[\"folds\"]\n",
    "    l2 = params[\"l2\"]\n",
    "    \n",
    "    splits=KFold(\n",
    "        n_splits=folds,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    foldperf={\n",
    "            'train_loss': [], \n",
    "            'train_acc': [], \n",
    "            'train_precision': [], \n",
    "            'train_recall': [],\n",
    "            'train_f1': [], \n",
    "            'val_loss': [], \n",
    "            'val_acc': [], \n",
    "            'val_precision': [], \n",
    "            'val_recall': [], \n",
    "            'val_f1': []\n",
    "    }\n",
    "    \n",
    "    model= nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "    test_dataset = DataLoader(images_test, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(images_train)))):\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "        \n",
    "        train_dataset = DataLoader(images_train, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_dataset = DataLoader(images_train, batch_size=batch_size, sampler=val_sampler)\n",
    "        train_loader = train_dataset\n",
    "        val_loader = val_dataset\n",
    "        test_loader = test_dataset\n",
    "\n",
    "        nb_classes = 2\n",
    "        \n",
    "        history = {\n",
    "            'train_loss': [], \n",
    "            'train_acc': [], \n",
    "            'train_precision': [], \n",
    "            'train_recall': [],\n",
    "            'train_f1': [], \n",
    "            'val_loss': [], \n",
    "            'val_acc': [], \n",
    "            'val_precision': [], \n",
    "            'val_recall': [], \n",
    "            'val_f1': []\n",
    "        }\n",
    "        \n",
    "        criterion = nn.BCEWithLogitsLoss()  \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)  \n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=patience, gamma=gamma)\n",
    "\n",
    "        print(\"++++++++ Training ++++++++\")\n",
    "        for epoch in range(epochs):\n",
    "            model.train()  \n",
    "\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            val_loss = 0\n",
    "            val_acc = 0\n",
    "\n",
    "            confusion_matrix_train = torch.zeros(nb_classes, nb_classes)\n",
    "            \n",
    "            #  For each batch \n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                print(\"=\",end=\"\")\n",
    "                \n",
    "                labels = labels.float()\n",
    "                images = images.to(device)  \n",
    "                labels = labels.to(device)  \n",
    "                \n",
    "                outputs = model(images).view(-1)  \n",
    "                pred = torch.sigmoid(outputs)\n",
    "                pred = torch.round(pred)\n",
    "                cur_train_loss = criterion(outputs, labels)  \n",
    "                cur_train_acc = (pred == labels).sum().item() / batch_size\n",
    "\n",
    "                for t, p in zip(labels.view(-1), pred.view(-1)):\n",
    "                    confusion_matrix_train[t.long(), p.long()] += 1\n",
    "\n",
    "             \n",
    "                cur_train_loss.backward()  \n",
    "                optimizer.step()           \n",
    "                optimizer.zero_grad()      \n",
    "\n",
    "      \n",
    "                train_loss += cur_train_loss         \n",
    "            print(\">\")\n",
    "        \n",
    "            precision_train, recall_train, f1_train, accuracy_train = getMetrics(confusion_matrix_train)\n",
    "            model.eval()  \n",
    "            confusion_matrix_val = torch.zeros(nb_classes, nb_classes)\n",
    "            with torch.no_grad():  \n",
    "                for images, labels in val_loader:\n",
    "                    \n",
    "                    labels = labels.float()\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    outputs = model(images).view(-1)\n",
    "\n",
    "                    # loss\n",
    "                    cur_valid_loss = criterion(outputs, labels)\n",
    "                    print(\"current lost :\",cur_valid_loss )\n",
    "                    val_loss += cur_valid_loss\n",
    "                    # acc\n",
    "                    pred = torch.sigmoid(outputs)\n",
    "                    pred = torch.round(pred)\n",
    "\n",
    "\n",
    "                    for t, p in zip(labels.view(-1), pred.view(-1)):\n",
    "                        confusion_matrix_val[t.long(), p.long()] += 1\n",
    "\n",
    "            precision_val, recall_val, f1_val, accuracy_val = getMetrics(confusion_matrix_val)\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "  \n",
    "            train_acc = accuracy_train\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "   \n",
    "            val_acc = accuracy_val\n",
    "\n",
    "            print(f\"Epoch:{epoch + 1} / {epochs},lr: {optimizer.param_groups[0]['lr']:.5f} train loss:{train_loss:.5f},train acc: {train_acc:.5f},valid loss:{val_loss:.5f}, valid acc:{val_acc:.5f}\")\n",
    "\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['train_precision'].append(precision_train)\n",
    "            history['train_recall'].append(recall_train)\n",
    "            history['train_f1'].append(f1_train)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['val_precision'].append(precision_val)\n",
    "            history['val_recall'].append(recall_val)\n",
    "            history['val_f1'].append(f1_val)\n",
    "        # for each epoch mean in one fold assign to single value\n",
    "        for key in history.keys():\n",
    "            foldperf[key].append(np.mean(torch.tensor(history[key], device = 'cpu').numpy()))\n",
    "\n",
    "\n",
    "    print('\\n\\nPerformance of {} fold cross validation'.format(folds))\n",
    "    print(\"average train results\")\n",
    "    print(\"loss      :\",np.mean(foldperf[\"train_loss\"]))\n",
    "    print(\"acc       :\",np.mean(foldperf[\"train_acc\"]))\n",
    "    print(\"precision :\",np.mean(foldperf[\"train_precision\"]))\n",
    "    print(\"recall    :\",np.mean(foldperf[\"train_recall\"]))\n",
    "    print(\"f1 score  :\",np.mean(foldperf[\"train_f1\"]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"average val results\")\n",
    "    print(\"loss      :\",np.mean(foldperf[\"val_loss\"]))\n",
    "    print(\"acc       :\",np.mean(foldperf[\"val_acc\"]))\n",
    "    print(\"precision :\",np.mean(foldperf[\"val_precision\"]))\n",
    "    print(\"recall    :\",np.mean(foldperf[\"val_recall\"]))\n",
    "    print(\"f1 score  :\",np.mean(foldperf[\"val_f1\"]))\n",
    "    \n",
    "    test_acc = 0\n",
    "    confusion_matrix_test = torch.zeros(nb_classes, nb_classes)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels = labels.float()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            \n",
    "            outputs = model(images)\n",
    "\n",
    "           \n",
    "            pred = torch.sigmoid(outputs)\n",
    "            pred = torch.round(pred)\n",
    "\n",
    "            test_acc += (pred == labels).sum().item()\n",
    "            for t, p in zip(labels.view(-1), pred.view(-1)):\n",
    "                confusion_matrix_test[t.long(), p.long()] += 1\n",
    "\n",
    "    precision_test, recall_test, f1_test, accuracy_test = getMetrics(confusion_matrix_test)\n",
    "    \n",
    "    print(f'\\n\\nTest Accuracy:', accuracy_test, 'Test Precision:', precision_test, 'Test Recall:', recall_test, 'Test F1:', f1_test)\n",
    "#     print('Train', confusion_matrix_train)\n",
    "#     print('Val', confusion_matrix_val)\n",
    "#     print('Test', confusion_matrix_test)\n",
    "\n",
    "    return foldperf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ploting(hist,folds):\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2,subplot_titles=['loss','accuracy'])\n",
    "    fig.add_trace(go.Scatter(x=[*range(1,folds+1)], y=hist[\"train_loss\"],name='train loss'),row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=[*range(1,folds+1)], y=hist[\"val_loss\"],name='val loss'),row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=[*range(1,folds+1)], y=hist[\"train_acc\"],name='train acc'),row=1, col=2)\n",
    "    fig.add_trace(go.Scatter(x=[*range(1,folds+1)], y=hist[\"val_acc\"],name='val acc'),row=1, col=2)\n",
    "    fig.update_layout(template='plotly_white');fig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0},height=300)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train cnn model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "params3 = {\n",
    "    \"model\" :cnn_3,\n",
    "    \"train\":final_train_dataset,\n",
    "    \"test\":test_dataset,\n",
    "    \"device\":device ,\n",
    "    \"lr\" :0.0002,\n",
    "    \"batch_size\":32,\n",
    "    \"epochs\":10,\n",
    "    \"gamma\": 0.5 ,\n",
    "    \"patience\": 7,\n",
    "    \"folds\": 5,\n",
    "    \"l2\":0.09\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "++++++++ Training ++++++++\n",
      "==============================>\n",
      "tp:326.0 | fp:237.0 | tn:200.0 | fn:190.0\n",
      "current lost : tensor(0.7536)\n",
      "current lost : tensor(0.7739)\n",
      "current lost : tensor(0.7167)\n",
      "current lost : tensor(0.7856)\n",
      "current lost : tensor(0.6870)\n",
      "current lost : tensor(0.6871)\n",
      "current lost : tensor(0.7405)\n",
      "current lost : tensor(0.8746)\n",
      "tp:116.0 | fp:123.0 | tn:0.0 | fn:0.0\n",
      "Epoch:1 / 10,lr: 0.00020 train loss:0.69451,train acc: 0.55194,valid loss:0.75238, valid acc:0.48536\n",
      "==============================>\n",
      "tp:403.0 | fp:247.0 | tn:190.0 | fn:113.0\n",
      "current lost : tensor(0.6595)\n",
      "current lost : tensor(0.6879)\n",
      "current lost : tensor(0.6830)\n",
      "current lost : tensor(0.6279)\n",
      "current lost : tensor(0.6669)\n",
      "current lost : tensor(0.6194)\n",
      "current lost : tensor(0.6237)\n",
      "current lost : tensor(0.8103)\n",
      "tp:43.0 | fp:24.0 | tn:99.0 | fn:73.0\n",
      "Epoch:2 / 10,lr: 0.00020 train loss:0.65667,train acc: 0.62225,valid loss:0.67232, valid acc:0.59414\n",
      "==============================>\n",
      "tp:361.0 | fp:186.0 | tn:251.0 | fn:155.0\n",
      "current lost : tensor(0.6331)\n",
      "current lost : tensor(0.6497)\n",
      "current lost : tensor(0.6216)\n",
      "current lost : tensor(0.6505)\n",
      "current lost : tensor(0.6100)\n",
      "current lost : tensor(0.6844)\n",
      "current lost : tensor(0.6026)\n",
      "current lost : tensor(0.5939)\n",
      "tp:76.0 | fp:46.0 | tn:77.0 | fn:40.0\n",
      "Epoch:3 / 10,lr: 0.00020 train loss:0.63430,train acc: 0.64218,valid loss:0.63072, valid acc:0.64017\n",
      "==============================>\n",
      "tp:346.0 | fp:169.0 | tn:268.0 | fn:170.0\n",
      "current lost : tensor(0.7262)\n",
      "current lost : tensor(0.6978)\n",
      "current lost : tensor(0.6744)\n",
      "current lost : tensor(0.5945)\n",
      "current lost : tensor(0.7671)\n",
      "current lost : tensor(0.6355)\n",
      "current lost : tensor(0.6880)\n",
      "current lost : tensor(0.8020)\n",
      "tp:108.0 | fp:96.0 | tn:27.0 | fn:8.0\n",
      "Epoch:4 / 10,lr: 0.00020 train loss:0.63702,train acc: 0.64428,valid loss:0.69819, valid acc:0.56485\n",
      "==============================>\n",
      "tp:366.0 | fp:164.0 | tn:273.0 | fn:150.0\n",
      "current lost : tensor(0.6014)\n",
      "current lost : tensor(0.7348)\n",
      "current lost : tensor(0.5809)\n",
      "current lost : tensor(0.6058)\n",
      "current lost : tensor(0.6597)\n",
      "current lost : tensor(0.7435)\n",
      "current lost : tensor(0.6209)\n",
      "current lost : tensor(0.6543)\n",
      "tp:84.0 | fp:52.0 | tn:71.0 | fn:32.0\n",
      "Epoch:5 / 10,lr: 0.00020 train loss:0.60442,train acc: 0.67051,valid loss:0.65014, valid acc:0.64854\n",
      "==============================>\n",
      "tp:358.0 | fp:148.0 | tn:289.0 | fn:158.0\n",
      "current lost : tensor(0.6558)\n",
      "current lost : tensor(0.6612)\n",
      "current lost : tensor(0.7502)\n",
      "current lost : tensor(0.6615)\n",
      "current lost : tensor(0.5998)\n",
      "current lost : tensor(0.6512)\n",
      "current lost : tensor(0.7335)\n",
      "current lost : tensor(0.6094)\n",
      "tp:102.0 | fp:75.0 | tn:48.0 | fn:14.0\n",
      "Epoch:6 / 10,lr: 0.00020 train loss:0.60670,train acc: 0.67891,valid loss:0.66532, valid acc:0.62762\n",
      "==============================>\n",
      "tp:375.0 | fp:157.0 | tn:280.0 | fn:141.0\n",
      "current lost : tensor(0.6411)\n",
      "current lost : tensor(0.6678)\n",
      "current lost : tensor(0.5539)\n",
      "current lost : tensor(0.6292)\n",
      "current lost : tensor(0.5171)\n",
      "current lost : tensor(0.6231)\n",
      "current lost : tensor(0.6696)\n",
      "current lost : tensor(0.5546)\n",
      "tp:97.0 | fp:60.0 | tn:63.0 | fn:19.0\n",
      "Epoch:7 / 10,lr: 0.00010 train loss:0.59314,train acc: 0.68730,valid loss:0.60705, valid acc:0.66946\n",
      "==============================>\n",
      "tp:404.0 | fp:151.0 | tn:286.0 | fn:112.0\n",
      "current lost : tensor(0.5662)\n",
      "current lost : tensor(0.6573)\n",
      "current lost : tensor(0.6436)\n",
      "current lost : tensor(0.4822)\n",
      "current lost : tensor(0.5693)\n",
      "current lost : tensor(0.6415)\n",
      "current lost : tensor(0.5079)\n",
      "current lost : tensor(0.5839)\n",
      "tp:77.0 | fp:31.0 | tn:92.0 | fn:39.0\n",
      "Epoch:8 / 10,lr: 0.00010 train loss:0.54751,train acc: 0.72403,valid loss:0.58148, valid acc:0.70711\n",
      "==============================>\n",
      "tp:387.0 | fp:136.0 | tn:301.0 | fn:129.0\n",
      "current lost : tensor(0.6578)\n",
      "current lost : tensor(0.5356)\n",
      "current lost : tensor(0.5254)\n",
      "current lost : tensor(0.6861)\n",
      "current lost : tensor(0.5564)\n",
      "current lost : tensor(0.4694)\n",
      "current lost : tensor(0.6531)\n",
      "current lost : tensor(0.6428)\n",
      "tp:64.0 | fp:26.0 | tn:97.0 | fn:52.0\n",
      "Epoch:9 / 10,lr: 0.00010 train loss:0.54841,train acc: 0.72193,valid loss:0.59084, valid acc:0.67364\n",
      "==============================>\n",
      "tp:419.0 | fp:126.0 | tn:311.0 | fn:97.0\n",
      "current lost : tensor(0.5482)\n",
      "current lost : tensor(0.5184)\n",
      "current lost : tensor(0.6717)\n",
      "current lost : tensor(0.6546)\n",
      "current lost : tensor(0.5634)\n",
      "current lost : tensor(0.5571)\n",
      "current lost : tensor(0.5192)\n",
      "current lost : tensor(0.4876)\n",
      "tp:75.0 | fp:24.0 | tn:99.0 | fn:41.0\n",
      "Epoch:10 / 10,lr: 0.00010 train loss:0.49958,train acc: 0.76600,valid loss:0.56503, valid acc:0.72803\n",
      "Fold 2\n",
      "++++++++ Training ++++++++\n",
      "==============================>\n",
      "tp:318.0 | fp:162.0 | tn:299.0 | fn:174.0\n",
      "current lost : tensor(0.5690)\n",
      "current lost : tensor(0.5729)\n",
      "current lost : tensor(0.5534)\n",
      "current lost : tensor(0.5726)\n",
      "current lost : tensor(0.5591)\n",
      "current lost : tensor(0.5622)\n",
      "current lost : tensor(0.5360)\n",
      "current lost : tensor(0.5929)\n",
      "tp:121.0 | fp:44.0 | tn:55.0 | fn:19.0\n",
      "Epoch:1 / 10,lr: 0.00020 train loss:0.62249,train acc: 0.64743,valid loss:0.56475, valid acc:0.73640\n",
      "==============================>\n",
      "tp:369.0 | fp:180.0 | tn:281.0 | fn:123.0\n",
      "current lost : tensor(0.6076)\n",
      "current lost : tensor(0.6288)\n",
      "current lost : tensor(0.5381)\n",
      "current lost : tensor(0.4847)\n",
      "current lost : tensor(0.6955)\n",
      "current lost : tensor(0.5686)\n",
      "current lost : tensor(0.4873)\n",
      "current lost : tensor(0.4900)\n",
      "tp:95.0 | fp:18.0 | tn:81.0 | fn:45.0\n",
      "Epoch:2 / 10,lr: 0.00020 train loss:0.58023,train acc: 0.68206,valid loss:0.56258, valid acc:0.73640\n",
      "==============================>\n",
      "tp:341.0 | fp:144.0 | tn:317.0 | fn:151.0\n",
      "current lost : tensor(0.5665)\n",
      "current lost : tensor(0.5655)\n",
      "current lost : tensor(0.5719)\n",
      "current lost : tensor(0.5296)\n",
      "current lost : tensor(0.5151)\n",
      "current lost : tensor(0.5163)\n",
      "current lost : tensor(0.5372)\n",
      "current lost : tensor(0.5697)\n",
      "tp:130.0 | fp:48.0 | tn:51.0 | fn:10.0\n",
      "Epoch:3 / 10,lr: 0.00020 train loss:0.57547,train acc: 0.69045,valid loss:0.54649, valid acc:0.75732\n",
      "==============================>\n",
      "tp:357.0 | fp:166.0 | tn:295.0 | fn:135.0\n",
      "current lost : tensor(0.4419)\n",
      "current lost : tensor(0.5240)\n",
      "current lost : tensor(0.5192)\n",
      "current lost : tensor(0.6278)\n",
      "current lost : tensor(0.6349)\n",
      "current lost : tensor(0.6441)\n",
      "current lost : tensor(0.5907)\n",
      "current lost : tensor(0.8545)\n",
      "tp:70.0 | fp:5.0 | tn:94.0 | fn:70.0\n",
      "Epoch:4 / 10,lr: 0.00020 train loss:0.59000,train acc: 0.68416,valid loss:0.60464, valid acc:0.68619\n",
      "==============================>\n",
      "tp:359.0 | fp:130.0 | tn:331.0 | fn:133.0\n",
      "current lost : tensor(0.5013)\n",
      "current lost : tensor(0.5183)\n",
      "current lost : tensor(0.5619)\n",
      "current lost : tensor(0.5824)\n",
      "current lost : tensor(0.4571)\n",
      "current lost : tensor(0.4516)\n",
      "current lost : tensor(0.3702)\n",
      "current lost : tensor(0.5590)\n",
      "tp:117.0 | fp:30.0 | tn:69.0 | fn:23.0\n",
      "Epoch:5 / 10,lr: 0.00020 train loss:0.55721,train acc: 0.72403,valid loss:0.50023, valid acc:0.77824\n",
      "==============================>\n",
      "tp:378.0 | fp:125.0 | tn:336.0 | fn:114.0\n",
      "current lost : tensor(0.4718)\n",
      "current lost : tensor(0.5785)\n",
      "current lost : tensor(0.4314)\n",
      "current lost : tensor(0.5027)\n",
      "current lost : tensor(0.5308)\n",
      "current lost : tensor(0.4526)\n",
      "current lost : tensor(0.4214)\n",
      "current lost : tensor(0.4319)\n",
      "tp:121.0 | fp:38.0 | tn:61.0 | fn:19.0\n",
      "Epoch:6 / 10,lr: 0.00020 train loss:0.53918,train acc: 0.74921,valid loss:0.47763, valid acc:0.76151\n",
      "==============================>\n",
      "tp:369.0 | fp:130.0 | tn:331.0 | fn:123.0\n",
      "current lost : tensor(0.5668)\n",
      "current lost : tensor(0.4646)\n",
      "current lost : tensor(0.5078)\n",
      "current lost : tensor(0.5107)\n",
      "current lost : tensor(0.5175)\n",
      "current lost : tensor(0.4110)\n",
      "current lost : tensor(0.4045)\n",
      "current lost : tensor(0.3756)\n",
      "tp:129.0 | fp:44.0 | tn:55.0 | fn:11.0\n",
      "Epoch:7 / 10,lr: 0.00010 train loss:0.53095,train acc: 0.73452,valid loss:0.46979, valid acc:0.76987\n",
      "==============================>\n",
      "tp:391.0 | fp:121.0 | tn:340.0 | fn:101.0\n",
      "current lost : tensor(0.4720)\n",
      "current lost : tensor(0.3904)\n",
      "current lost : tensor(0.5840)\n",
      "current lost : tensor(0.4959)\n",
      "current lost : tensor(0.5032)\n",
      "current lost : tensor(0.5679)\n",
      "current lost : tensor(0.4985)\n",
      "current lost : tensor(0.3641)\n",
      "tp:133.0 | fp:57.0 | tn:42.0 | fn:7.0\n",
      "Epoch:8 / 10,lr: 0.00010 train loss:0.49445,train acc: 0.76705,valid loss:0.48450, valid acc:0.73222\n",
      "==============================>\n",
      "tp:395.0 | fp:106.0 | tn:355.0 | fn:97.0\n",
      "current lost : tensor(0.4819)\n",
      "current lost : tensor(0.4930)\n",
      "current lost : tensor(0.4248)\n",
      "current lost : tensor(0.4328)\n",
      "current lost : tensor(0.4854)\n",
      "current lost : tensor(0.4136)\n",
      "current lost : tensor(0.4615)\n",
      "current lost : tensor(0.7472)\n",
      "tp:134.0 | fp:50.0 | tn:49.0 | fn:6.0\n",
      "Epoch:9 / 10,lr: 0.00010 train loss:0.47603,train acc: 0.78699,valid loss:0.49251, valid acc:0.76569\n",
      "==============================>\n",
      "tp:380.0 | fp:105.0 | tn:356.0 | fn:112.0\n",
      "current lost : tensor(0.4228)\n",
      "current lost : tensor(0.3738)\n",
      "current lost : tensor(0.5622)\n",
      "current lost : tensor(0.4993)\n",
      "current lost : tensor(0.4582)\n",
      "current lost : tensor(0.4165)\n",
      "current lost : tensor(0.4344)\n",
      "current lost : tensor(0.4506)\n",
      "tp:106.0 | fp:16.0 | tn:83.0 | fn:34.0\n",
      "Epoch:10 / 10,lr: 0.00010 train loss:0.47561,train acc: 0.77230,valid loss:0.45223, valid acc:0.79079\n",
      "Fold 3\n",
      "++++++++ Training ++++++++\n",
      "==============================>\n",
      "tp:371.0 | fp:136.0 | tn:310.0 | fn:137.0\n",
      "current lost : tensor(0.6237)\n",
      "current lost : tensor(0.6258)\n",
      "current lost : tensor(0.4708)\n",
      "current lost : tensor(0.4154)\n",
      "current lost : tensor(0.5928)\n",
      "current lost : tensor(0.4243)\n",
      "current lost : tensor(0.4922)\n",
      "current lost : tensor(0.7420)\n",
      "tp:101.0 | fp:34.0 | tn:80.0 | fn:23.0\n",
      "Epoch:1 / 10,lr: 0.00020 train loss:0.55531,train acc: 0.71384,valid loss:0.54838, valid acc:0.76050\n",
      "==============================>\n",
      "tp:388.0 | fp:134.0 | tn:312.0 | fn:120.0\n",
      "current lost : tensor(0.5746)\n",
      "current lost : tensor(0.4362)\n",
      "current lost : tensor(0.3944)\n",
      "current lost : tensor(0.4739)\n",
      "current lost : tensor(0.5867)\n",
      "current lost : tensor(0.7062)\n",
      "current lost : tensor(0.5746)\n",
      "current lost : tensor(0.4425)\n",
      "tp:104.0 | fp:40.0 | tn:74.0 | fn:20.0\n",
      "Epoch:2 / 10,lr: 0.00020 train loss:0.53957,train acc: 0.73375,valid loss:0.52363, valid acc:0.74790\n",
      "==============================>\n",
      "tp:414.0 | fp:132.0 | tn:314.0 | fn:94.0\n",
      "current lost : tensor(0.6223)\n",
      "current lost : tensor(0.8661)\n",
      "current lost : tensor(0.5573)\n",
      "current lost : tensor(0.6040)\n",
      "current lost : tensor(0.4586)\n",
      "current lost : tensor(0.4622)\n",
      "current lost : tensor(0.4077)\n",
      "current lost : tensor(0.9810)\n",
      "tp:65.0 | fp:8.0 | tn:106.0 | fn:59.0\n",
      "Epoch:3 / 10,lr: 0.00020 train loss:0.49909,train acc: 0.76310,valid loss:0.61989, valid acc:0.71849\n",
      "==============================>\n",
      "tp:405.0 | fp:118.0 | tn:328.0 | fn:103.0\n",
      "current lost : tensor(0.6567)\n",
      "current lost : tensor(0.7438)\n",
      "current lost : tensor(0.5462)\n",
      "current lost : tensor(0.5227)\n",
      "current lost : tensor(0.7848)\n",
      "current lost : tensor(0.7287)\n",
      "current lost : tensor(0.8735)\n",
      "current lost : tensor(0.2989)\n",
      "tp:120.0 | fp:73.0 | tn:41.0 | fn:4.0\n",
      "Epoch:4 / 10,lr: 0.00020 train loss:0.48537,train acc: 0.76834,valid loss:0.64442, valid acc:0.67647\n",
      "==============================>\n",
      "tp:398.0 | fp:115.0 | tn:331.0 | fn:110.0\n",
      "current lost : tensor(0.6165)\n",
      "current lost : tensor(0.5119)\n",
      "current lost : tensor(0.4140)\n",
      "current lost : tensor(0.5466)\n",
      "current lost : tensor(0.3947)\n",
      "current lost : tensor(0.5545)\n",
      "current lost : tensor(0.3512)\n",
      "current lost : tensor(0.5435)\n",
      "tp:92.0 | fp:28.0 | tn:86.0 | fn:32.0\n",
      "Epoch:5 / 10,lr: 0.00020 train loss:0.50168,train acc: 0.76415,valid loss:0.49161, valid acc:0.74790\n",
      "==============================>\n",
      "tp:400.0 | fp:116.0 | tn:330.0 | fn:108.0\n",
      "current lost : tensor(0.4793)\n",
      "current lost : tensor(0.5633)\n",
      "current lost : tensor(0.6843)\n",
      "current lost : tensor(0.4392)\n",
      "current lost : tensor(0.6946)\n",
      "current lost : tensor(0.4812)\n",
      "current lost : tensor(0.4824)\n",
      "current lost : tensor(0.4027)\n",
      "tp:110.0 | fp:57.0 | tn:57.0 | fn:14.0\n",
      "Epoch:6 / 10,lr: 0.00020 train loss:0.50228,train acc: 0.76520,valid loss:0.52840, valid acc:0.70168\n",
      "==============================>\n",
      "tp:407.0 | fp:112.0 | tn:334.0 | fn:101.0\n",
      "current lost : tensor(0.4264)\n",
      "current lost : tensor(0.4793)\n",
      "current lost : tensor(0.4640)\n",
      "current lost : tensor(0.4874)\n",
      "current lost : tensor(0.5502)\n",
      "current lost : tensor(0.6119)\n",
      "current lost : tensor(0.7517)\n",
      "current lost : tensor(0.4206)\n",
      "tp:84.0 | fp:23.0 | tn:91.0 | fn:40.0\n",
      "Epoch:7 / 10,lr: 0.00010 train loss:0.49805,train acc: 0.77673,valid loss:0.52394, valid acc:0.73529\n",
      "==============================>\n",
      "tp:407.0 | fp:101.0 | tn:345.0 | fn:101.0\n",
      "current lost : tensor(0.5444)\n",
      "current lost : tensor(0.8733)\n",
      "current lost : tensor(0.5315)\n",
      "current lost : tensor(0.7102)\n",
      "current lost : tensor(0.7007)\n",
      "current lost : tensor(0.7697)\n",
      "current lost : tensor(0.5816)\n",
      "current lost : tensor(0.3397)\n",
      "tp:120.0 | fp:76.0 | tn:38.0 | fn:4.0\n",
      "Epoch:8 / 10,lr: 0.00010 train loss:0.45457,train acc: 0.78826,valid loss:0.63138, valid acc:0.66387\n",
      "==============================>\n",
      "tp:420.0 | fp:103.0 | tn:343.0 | fn:88.0\n",
      "current lost : tensor(0.4559)\n",
      "current lost : tensor(0.5769)\n",
      "current lost : tensor(0.5037)\n",
      "current lost : tensor(0.5190)\n",
      "current lost : tensor(0.5985)\n",
      "current lost : tensor(0.5982)\n",
      "current lost : tensor(0.7220)\n",
      "current lost : tensor(0.2609)\n",
      "tp:121.0 | fp:67.0 | tn:47.0 | fn:3.0\n",
      "Epoch:9 / 10,lr: 0.00010 train loss:0.42700,train acc: 0.79979,valid loss:0.52940, valid acc:0.70588\n",
      "==============================>\n",
      "tp:429.0 | fp:89.0 | tn:357.0 | fn:79.0\n",
      "current lost : tensor(0.4053)\n",
      "current lost : tensor(0.4247)\n",
      "current lost : tensor(0.2835)\n",
      "current lost : tensor(0.7184)\n",
      "current lost : tensor(0.5185)\n",
      "current lost : tensor(0.3894)\n",
      "current lost : tensor(0.4811)\n",
      "current lost : tensor(0.2717)\n",
      "tp:98.0 | fp:25.0 | tn:89.0 | fn:26.0\n",
      "Epoch:10 / 10,lr: 0.00010 train loss:0.40936,train acc: 0.82390,valid loss:0.43658, valid acc:0.78571\n",
      "Fold 4\n",
      "++++++++ Training ++++++++\n",
      "==============================>\n",
      "tp:397.0 | fp:142.0 | tn:307.0 | fn:108.0\n",
      "current lost : tensor(0.4191)\n",
      "current lost : tensor(0.4706)\n",
      "current lost : tensor(0.5262)\n",
      "current lost : tensor(0.5552)\n",
      "current lost : tensor(0.4616)\n",
      "current lost : tensor(0.4440)\n",
      "current lost : tensor(0.4508)\n",
      "current lost : tensor(0.6344)\n",
      "tp:122.0 | fp:47.0 | tn:64.0 | fn:5.0\n",
      "Epoch:1 / 10,lr: 0.00020 train loss:0.54237,train acc: 0.73795,valid loss:0.49524, valid acc:0.78151\n",
      "==============================>\n",
      "tp:413.0 | fp:115.0 | tn:334.0 | fn:92.0\n",
      "current lost : tensor(0.3522)\n",
      "current lost : tensor(0.5086)\n",
      "current lost : tensor(0.4915)\n",
      "current lost : tensor(0.4140)\n",
      "current lost : tensor(0.4683)\n",
      "current lost : tensor(0.4144)\n",
      "current lost : tensor(0.4510)\n",
      "current lost : tensor(0.4490)\n",
      "tp:94.0 | fp:18.0 | tn:93.0 | fn:33.0\n",
      "Epoch:2 / 10,lr: 0.00020 train loss:0.47057,train acc: 0.78302,valid loss:0.44363, valid acc:0.78571\n",
      "==============================>\n",
      "tp:401.0 | fp:123.0 | tn:326.0 | fn:104.0\n",
      "current lost : tensor(0.5669)\n",
      "current lost : tensor(0.4461)\n",
      "current lost : tensor(0.5041)\n",
      "current lost : tensor(0.5674)\n",
      "current lost : tensor(0.4967)\n",
      "current lost : tensor(0.4557)\n",
      "current lost : tensor(0.8132)\n",
      "current lost : tensor(0.4476)\n",
      "tp:123.0 | fp:66.0 | tn:45.0 | fn:4.0\n",
      "Epoch:3 / 10,lr: 0.00020 train loss:0.48827,train acc: 0.76205,valid loss:0.53720, valid acc:0.70588\n",
      "==============================>\n",
      "tp:413.0 | fp:118.0 | tn:331.0 | fn:92.0\n",
      "current lost : tensor(0.4810)\n",
      "current lost : tensor(0.5589)\n",
      "current lost : tensor(0.4084)\n",
      "current lost : tensor(0.4830)\n",
      "current lost : tensor(0.3732)\n",
      "current lost : tensor(0.4298)\n",
      "current lost : tensor(0.5531)\n",
      "current lost : tensor(0.5518)\n",
      "tp:122.0 | fp:52.0 | tn:59.0 | fn:5.0\n",
      "Epoch:4 / 10,lr: 0.00020 train loss:0.46689,train acc: 0.77987,valid loss:0.47988, valid acc:0.76050\n",
      "==============================>\n",
      "tp:397.0 | fp:114.0 | tn:335.0 | fn:108.0\n",
      "current lost : tensor(0.4402)\n",
      "current lost : tensor(0.3981)\n",
      "current lost : tensor(0.5118)\n",
      "current lost : tensor(0.5859)\n",
      "current lost : tensor(0.4381)\n",
      "current lost : tensor(0.4067)\n",
      "current lost : tensor(0.5374)\n",
      "current lost : tensor(0.6530)\n",
      "tp:118.0 | fp:52.0 | tn:59.0 | fn:9.0\n",
      "Epoch:5 / 10,lr: 0.00020 train loss:0.48150,train acc: 0.76730,valid loss:0.49640, valid acc:0.74370\n",
      "==============================>\n",
      "tp:423.0 | fp:97.0 | tn:352.0 | fn:82.0\n",
      "current lost : tensor(0.3979)\n",
      "current lost : tensor(0.4296)\n",
      "current lost : tensor(0.4287)\n",
      "current lost : tensor(0.5509)\n",
      "current lost : tensor(0.5813)\n",
      "current lost : tensor(0.5825)\n",
      "current lost : tensor(0.5285)\n",
      "current lost : tensor(0.4524)\n",
      "tp:121.0 | fp:47.0 | tn:64.0 | fn:6.0\n",
      "Epoch:6 / 10,lr: 0.00020 train loss:0.42612,train acc: 0.81237,valid loss:0.49398, valid acc:0.77731\n",
      "==============================>\n",
      "tp:408.0 | fp:116.0 | tn:333.0 | fn:97.0\n",
      "current lost : tensor(0.4362)\n",
      "current lost : tensor(0.3869)\n",
      "current lost : tensor(0.4494)\n",
      "current lost : tensor(0.4275)\n",
      "current lost : tensor(0.4108)\n",
      "current lost : tensor(0.3978)\n",
      "current lost : tensor(0.3395)\n",
      "current lost : tensor(0.3614)\n",
      "tp:99.0 | fp:15.0 | tn:96.0 | fn:28.0\n",
      "Epoch:7 / 10,lr: 0.00010 train loss:0.47715,train acc: 0.77673,valid loss:0.40118, valid acc:0.81933\n",
      "==============================>\n",
      "tp:432.0 | fp:92.0 | tn:357.0 | fn:73.0\n",
      "current lost : tensor(0.3309)\n",
      "current lost : tensor(0.3898)\n",
      "current lost : tensor(0.4338)\n",
      "current lost : tensor(0.2974)\n",
      "current lost : tensor(0.3327)\n",
      "current lost : tensor(0.2758)\n",
      "current lost : tensor(0.4154)\n",
      "current lost : tensor(0.5130)\n",
      "tp:104.0 | fp:16.0 | tn:95.0 | fn:23.0\n",
      "Epoch:8 / 10,lr: 0.00010 train loss:0.37963,train acc: 0.82704,valid loss:0.37359, valid acc:0.83613\n",
      "==============================>\n",
      "tp:436.0 | fp:88.0 | tn:361.0 | fn:69.0\n",
      "current lost : tensor(0.5808)\n",
      "current lost : tensor(0.3103)\n",
      "current lost : tensor(0.6825)\n",
      "current lost : tensor(0.4696)\n",
      "current lost : tensor(0.4342)\n",
      "current lost : tensor(0.6604)\n",
      "current lost : tensor(0.4757)\n",
      "current lost : tensor(0.4443)\n",
      "tp:120.0 | fp:53.0 | tn:58.0 | fn:7.0\n",
      "Epoch:9 / 10,lr: 0.00010 train loss:0.36514,train acc: 0.83543,valid loss:0.50724, valid acc:0.74790\n",
      "==============================>\n",
      "tp:424.0 | fp:95.0 | tn:354.0 | fn:81.0\n",
      "current lost : tensor(0.3993)\n",
      "current lost : tensor(0.2783)\n",
      "current lost : tensor(0.3752)\n",
      "current lost : tensor(0.3892)\n",
      "current lost : tensor(0.3544)\n",
      "current lost : tensor(0.3980)\n",
      "current lost : tensor(0.5597)\n",
      "current lost : tensor(0.3597)\n",
      "tp:118.0 | fp:34.0 | tn:77.0 | fn:9.0\n",
      "Epoch:10 / 10,lr: 0.00010 train loss:0.38719,train acc: 0.81551,valid loss:0.38923, valid acc:0.81933\n",
      "Fold 5\n",
      "++++++++ Training ++++++++\n",
      "==============================>\n",
      "tp:404.0 | fp:131.0 | tn:316.0 | fn:103.0\n",
      "current lost : tensor(0.4125)\n",
      "current lost : tensor(0.4532)\n",
      "current lost : tensor(0.5158)\n",
      "current lost : tensor(0.4650)\n",
      "current lost : tensor(0.5752)\n",
      "current lost : tensor(0.4039)\n",
      "current lost : tensor(0.5851)\n",
      "current lost : tensor(0.2983)\n",
      "tp:119.0 | fp:52.0 | tn:61.0 | fn:6.0\n",
      "Epoch:1 / 10,lr: 0.00020 train loss:0.50161,train acc: 0.75472,valid loss:0.46364, valid acc:0.75630\n",
      "==============================>\n",
      "tp:407.0 | fp:111.0 | tn:336.0 | fn:100.0\n",
      "current lost : tensor(0.4293)\n",
      "current lost : tensor(0.3857)\n",
      "current lost : tensor(0.4953)\n",
      "current lost : tensor(0.3969)\n",
      "current lost : tensor(0.4001)\n",
      "current lost : tensor(0.4091)\n",
      "current lost : tensor(0.4882)\n",
      "current lost : tensor(0.4447)\n",
      "tp:101.0 | fp:26.0 | tn:87.0 | fn:24.0\n",
      "Epoch:2 / 10,lr: 0.00020 train loss:0.45452,train acc: 0.77883,valid loss:0.43117, valid acc:0.78992\n",
      "==============================>\n",
      "tp:415.0 | fp:93.0 | tn:354.0 | fn:92.0\n",
      "current lost : tensor(0.4586)\n",
      "current lost : tensor(0.6050)\n",
      "current lost : tensor(0.3328)\n",
      "current lost : tensor(0.6095)\n",
      "current lost : tensor(0.6521)\n",
      "current lost : tensor(0.5320)\n",
      "current lost : tensor(0.7755)\n",
      "current lost : tensor(0.2383)\n",
      "tp:119.0 | fp:59.0 | tn:54.0 | fn:6.0\n",
      "Epoch:3 / 10,lr: 0.00020 train loss:0.40974,train acc: 0.80608,valid loss:0.52547, valid acc:0.72689\n",
      "==============================>\n",
      "tp:417.0 | fp:109.0 | tn:338.0 | fn:90.0\n",
      "current lost : tensor(0.4203)\n",
      "current lost : tensor(0.3364)\n",
      "current lost : tensor(0.4786)\n",
      "current lost : tensor(0.4746)\n",
      "current lost : tensor(0.3603)\n",
      "current lost : tensor(0.5794)\n",
      "current lost : tensor(0.3761)\n",
      "current lost : tensor(0.2674)\n",
      "tp:96.0 | fp:18.0 | tn:95.0 | fn:29.0\n",
      "Epoch:4 / 10,lr: 0.00020 train loss:0.45381,train acc: 0.79140,valid loss:0.41164, valid acc:0.80252\n",
      "==============================>\n",
      "tp:421.0 | fp:115.0 | tn:332.0 | fn:86.0\n",
      "current lost : tensor(0.3645)\n",
      "current lost : tensor(0.4533)\n",
      "current lost : tensor(0.3985)\n",
      "current lost : tensor(0.4447)\n",
      "current lost : tensor(0.5718)\n",
      "current lost : tensor(0.5345)\n",
      "current lost : tensor(0.4521)\n",
      "current lost : tensor(0.3452)\n",
      "tp:117.0 | fp:42.0 | tn:71.0 | fn:8.0\n",
      "Epoch:5 / 10,lr: 0.00020 train loss:0.44209,train acc: 0.78931,valid loss:0.44558, valid acc:0.78992\n",
      "==============================>\n",
      "tp:429.0 | fp:95.0 | tn:352.0 | fn:78.0\n",
      "current lost : tensor(0.4543)\n",
      "current lost : tensor(0.4695)\n",
      "current lost : tensor(0.4687)\n",
      "current lost : tensor(0.4178)\n",
      "current lost : tensor(0.4487)\n",
      "current lost : tensor(0.5371)\n",
      "current lost : tensor(0.3781)\n",
      "current lost : tensor(0.4087)\n",
      "tp:95.0 | fp:17.0 | tn:96.0 | fn:30.0\n",
      "Epoch:6 / 10,lr: 0.00020 train loss:0.42571,train acc: 0.81866,valid loss:0.44786, valid acc:0.80252\n",
      "==============================>\n",
      "tp:419.0 | fp:93.0 | tn:354.0 | fn:88.0\n",
      "current lost : tensor(0.7075)\n",
      "current lost : tensor(0.5187)\n",
      "current lost : tensor(0.8417)\n",
      "current lost : tensor(1.0757)\n",
      "current lost : tensor(0.7663)\n",
      "current lost : tensor(0.6102)\n",
      "current lost : tensor(0.5357)\n",
      "current lost : tensor(1.1364)\n",
      "tp:123.0 | fp:92.0 | tn:21.0 | fn:2.0\n",
      "Epoch:7 / 10,lr: 0.00010 train loss:0.42528,train acc: 0.81027,valid loss:0.77402, valid acc:0.60504\n",
      "==============================>\n",
      "tp:424.0 | fp:111.0 | tn:336.0 | fn:83.0\n",
      "current lost : tensor(0.5843)\n",
      "current lost : tensor(0.5122)\n",
      "current lost : tensor(0.3324)\n",
      "current lost : tensor(0.6340)\n",
      "current lost : tensor(0.5231)\n",
      "current lost : tensor(0.4266)\n",
      "current lost : tensor(0.4536)\n",
      "current lost : tensor(0.4428)\n",
      "tp:116.0 | fp:51.0 | tn:62.0 | fn:9.0\n",
      "Epoch:8 / 10,lr: 0.00010 train loss:0.43135,train acc: 0.79665,valid loss:0.48861, valid acc:0.74790\n",
      "==============================>\n",
      "tp:453.0 | fp:75.0 | tn:372.0 | fn:54.0\n",
      "current lost : tensor(0.3543)\n",
      "current lost : tensor(0.4032)\n",
      "current lost : tensor(0.3328)\n",
      "current lost : tensor(0.3006)\n",
      "current lost : tensor(0.4156)\n",
      "current lost : tensor(0.3865)\n",
      "current lost : tensor(0.5521)\n",
      "current lost : tensor(0.4094)\n",
      "tp:98.0 | fp:16.0 | tn:97.0 | fn:27.0\n",
      "Epoch:9 / 10,lr: 0.00010 train loss:0.33653,train acc: 0.86478,valid loss:0.39432, valid acc:0.81933\n",
      "==============================>\n",
      "tp:454.0 | fp:72.0 | tn:375.0 | fn:53.0\n",
      "current lost : tensor(0.5425)\n",
      "current lost : tensor(0.3896)\n",
      "current lost : tensor(0.2986)\n",
      "current lost : tensor(0.3071)\n",
      "current lost : tensor(0.2927)\n",
      "current lost : tensor(0.3870)\n",
      "current lost : tensor(0.4881)\n",
      "current lost : tensor(0.2420)\n",
      "tp:110.0 | fp:28.0 | tn:85.0 | fn:15.0\n",
      "Epoch:10 / 10,lr: 0.00010 train loss:0.30861,train acc: 0.86897,valid loss:0.36845, valid acc:0.81933\n",
      "\n",
      "\n",
      "Performance of 5 fold cross validation\n",
      "average train results\n",
      "loss      : 0.5002049\n",
      "acc       : 0.75243074\n",
      "precision : 0.75835544\n",
      "recall    : 0.7843248\n",
      "f1 score  : 0.7708522\n",
      "average val results\n",
      "loss      : 0.52829665\n",
      "acc       : 0.7306846\n",
      "precision : 0.72795486\n",
      "recall    : 0.82874995\n",
      "f1 score  : 0.7614126\n",
      "tp:68.0 | fp:28.0 | tn:41.0 | fn:13.0\n",
      "\n",
      "\n",
      "Test Accuracy: tensor(0.7267) Test Precision: tensor(0.7083) Test Recall: tensor(0.8395) Test F1: tensor(0.7684)\n"
     ]
    }
   ],
   "source": [
    "hist1 = train_model(params3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "params4 = {\n",
    "    \"model\" :vgg19,\n",
    "    \"train\":final_train_dataset,\n",
    "    \"test\":test_dataset,\n",
    "    \"device\":device ,\n",
    "    \"lr\" :0.02,\n",
    "    \"batch_size\":32,\n",
    "    \"epochs\":10,\n",
    "    \"gamma\": 0.5 ,\n",
    "    \"patience\": 7,\n",
    "    \"folds\": 5,\n",
    "    \"l2\":0.09\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "++++++++ Training ++++++++\n",
      "==============================>\n",
      "tp:294.0 | fp:201.0 | tn:236.0 | fn:222.0\n",
      "current lost : tensor(0.8969)\n",
      "current lost : tensor(1.0963)\n",
      "current lost : tensor(1.3297)\n",
      "current lost : tensor(1.4531)\n",
      "current lost : tensor(1.6167)\n",
      "current lost : tensor(1.2772)\n",
      "current lost : tensor(0.7358)\n",
      "current lost : tensor(1.4011)\n",
      "tp:115.0 | fp:123.0 | tn:0.0 | fn:1.0\n",
      "Epoch:1 / 10,lr: 0.02000 train loss:3.36917,train acc: 0.55614,valid loss:1.22584, valid acc:0.48117\n",
      "==============================>\n",
      "tp:333.0 | fp:222.0 | tn:215.0 | fn:183.0\n",
      "current lost : tensor(0.6684)\n",
      "current lost : tensor(0.7867)\n",
      "current lost : tensor(0.8512)\n",
      "current lost : tensor(0.9142)\n",
      "current lost : tensor(0.7176)\n",
      "current lost : tensor(0.9038)\n",
      "current lost : tensor(1.2136)\n",
      "current lost : tensor(0.9176)\n",
      "tp:88.0 | fp:82.0 | tn:41.0 | fn:28.0\n",
      "Epoch:2 / 10,lr: 0.02000 train loss:1.01163,train acc: 0.57503,valid loss:0.87164, valid acc:0.53975\n",
      "==============================>\n",
      "tp:321.0 | fp:212.0 | tn:225.0 | fn:195.0\n",
      "current lost : tensor(1.3825)\n",
      "current lost : tensor(0.8285)\n",
      "current lost : tensor(1.0457)\n",
      "current lost : tensor(0.8746)\n",
      "current lost : tensor(1.3178)\n",
      "current lost : tensor(1.1454)\n",
      "current lost : tensor(1.2093)\n",
      "current lost : tensor(0.9204)\n",
      "tp:5.0 | fp:1.0 | tn:122.0 | fn:111.0\n",
      "Epoch:3 / 10,lr: 0.02000 train loss:0.86928,train acc: 0.57293,valid loss:1.09053, valid acc:0.53138\n",
      "==============================>\n",
      "tp:333.0 | fp:214.0 | tn:223.0 | fn:183.0\n",
      "current lost : tensor(0.8281)\n",
      "current lost : tensor(0.7559)\n",
      "current lost : tensor(0.9605)\n",
      "current lost : tensor(0.7739)\n",
      "current lost : tensor(0.6621)\n",
      "current lost : tensor(1.3536)\n",
      "current lost : tensor(1.0196)\n",
      "current lost : tensor(1.2793)\n",
      "tp:7.0 | fp:2.0 | tn:121.0 | fn:109.0\n",
      "Epoch:4 / 10,lr: 0.02000 train loss:0.85738,train acc: 0.58342,valid loss:0.95412, valid acc:0.53556\n",
      "==============================>\n",
      "tp:340.0 | fp:218.0 | tn:219.0 | fn:176.0\n",
      "current lost : tensor(0.8500)\n",
      "current lost : tensor(0.6140)\n",
      "current lost : tensor(0.7834)\n",
      "current lost : tensor(0.7131)\n",
      "current lost : tensor(0.6897)\n",
      "current lost : tensor(0.5374)\n",
      "current lost : tensor(0.7288)\n",
      "current lost : tensor(0.5381)\n",
      "tp:21.0 | fp:4.0 | tn:119.0 | fn:95.0\n",
      "Epoch:5 / 10,lr: 0.02000 train loss:0.78612,train acc: 0.58657,valid loss:0.68180, valid acc:0.58577\n",
      "==============================>\n",
      "tp:338.0 | fp:231.0 | tn:206.0 | fn:178.0\n",
      "current lost : tensor(0.6048)\n",
      "current lost : tensor(0.6412)\n",
      "current lost : tensor(0.6097)\n",
      "current lost : tensor(0.5952)\n",
      "current lost : tensor(0.6374)\n",
      "current lost : tensor(0.6746)\n",
      "current lost : tensor(0.7234)\n",
      "current lost : tensor(0.7266)\n",
      "tp:83.0 | fp:56.0 | tn:67.0 | fn:33.0\n",
      "Epoch:6 / 10,lr: 0.02000 train loss:0.73949,train acc: 0.57083,valid loss:0.65160, valid acc:0.62762\n",
      "==============================>\n",
      "tp:307.0 | fp:227.0 | tn:210.0 | fn:209.0\n",
      "current lost : tensor(1.0247)\n",
      "current lost : tensor(1.5252)\n",
      "current lost : tensor(0.9274)\n",
      "current lost : tensor(0.9045)\n",
      "current lost : tensor(0.9131)\n",
      "current lost : tensor(1.3882)\n",
      "current lost : tensor(1.0928)\n",
      "current lost : tensor(1.3736)\n",
      "tp:1.0 | fp:0.0 | tn:123.0 | fn:115.0\n",
      "Epoch:7 / 10,lr: 0.01000 train loss:0.99637,train acc: 0.54250,valid loss:1.14370, valid acc:0.51883\n",
      "==============================>\n",
      "tp:316.0 | fp:202.0 | tn:235.0 | fn:200.0\n",
      "current lost : tensor(0.6798)\n",
      "current lost : tensor(0.5897)\n",
      "current lost : tensor(0.6645)\n",
      "current lost : tensor(0.5301)\n",
      "current lost : tensor(0.6937)\n",
      "current lost : tensor(0.6991)\n",
      "current lost : tensor(0.5673)\n",
      "current lost : tensor(0.5610)\n",
      "tp:99.0 | fp:66.0 | tn:57.0 | fn:17.0\n",
      "Epoch:8 / 10,lr: 0.01000 train loss:0.79288,train acc: 0.57817,valid loss:0.62316, valid acc:0.65272\n",
      "==============================>\n",
      "tp:369.0 | fp:204.0 | tn:233.0 | fn:147.0\n",
      "current lost : tensor(0.6415)\n",
      "current lost : tensor(0.7151)\n",
      "current lost : tensor(0.6701)\n",
      "current lost : tensor(0.6263)\n",
      "current lost : tensor(0.6343)\n",
      "current lost : tensor(0.6750)\n",
      "current lost : tensor(0.6468)\n",
      "current lost : tensor(0.5632)\n",
      "tp:90.0 | fp:61.0 | tn:62.0 | fn:26.0\n",
      "Epoch:9 / 10,lr: 0.01000 train loss:0.66865,train acc: 0.63169,valid loss:0.64656, valid acc:0.63598\n",
      "==============================>\n",
      "tp:342.0 | fp:193.0 | tn:244.0 | fn:174.0\n",
      "current lost : tensor(0.6655)\n",
      "current lost : tensor(0.6623)\n",
      "current lost : tensor(0.7093)\n",
      "current lost : tensor(0.6333)\n",
      "current lost : tensor(0.6495)\n",
      "current lost : tensor(0.5926)\n",
      "current lost : tensor(0.6454)\n",
      "current lost : tensor(0.7306)\n",
      "tp:65.0 | fp:36.0 | tn:87.0 | fn:51.0\n",
      "Epoch:10 / 10,lr: 0.01000 train loss:0.64562,train acc: 0.61490,valid loss:0.66108, valid acc:0.63598\n",
      "Fold 2\n",
      "++++++++ Training ++++++++\n",
      "==============================>\n",
      "tp:290.0 | fp:242.0 | tn:219.0 | fn:202.0\n",
      "current lost : tensor(3.0080)\n",
      "current lost : tensor(1.1396)\n",
      "current lost : tensor(1.1911)\n",
      "current lost : tensor(1.5432)\n",
      "current lost : tensor(1.6577)\n",
      "current lost : tensor(2.7975)\n",
      "current lost : tensor(2.1884)\n",
      "current lost : tensor(1.8740)\n",
      "tp:139.0 | fp:96.0 | tn:3.0 | fn:1.0\n",
      "Epoch:1 / 10,lr: 0.02000 train loss:4.51988,train acc: 0.53410,valid loss:1.92494, valid acc:0.59414\n",
      "==============================>\n",
      "tp:265.0 | fp:213.0 | tn:248.0 | fn:227.0\n",
      "current lost : tensor(1.1128)\n",
      "current lost : tensor(1.0954)\n",
      "current lost : tensor(1.1625)\n",
      "current lost : tensor(0.9619)\n",
      "current lost : tensor(0.5636)\n",
      "current lost : tensor(1.5696)\n",
      "current lost : tensor(0.6678)\n",
      "current lost : tensor(1.2186)\n",
      "tp:140.0 | fp:98.0 | tn:1.0 | fn:0.0\n",
      "Epoch:2 / 10,lr: 0.02000 train loss:2.01668,train acc: 0.53830,valid loss:1.04403, valid acc:0.58996\n",
      "==============================>\n",
      "tp:279.0 | fp:210.0 | tn:251.0 | fn:213.0\n",
      "current lost : tensor(0.6577)\n",
      "current lost : tensor(0.6849)\n",
      "current lost : tensor(0.6077)\n",
      "current lost : tensor(0.5440)\n",
      "current lost : tensor(0.7481)\n",
      "current lost : tensor(0.5457)\n",
      "current lost : tensor(0.7877)\n",
      "current lost : tensor(0.4592)\n",
      "tp:78.0 | fp:26.0 | tn:73.0 | fn:62.0\n",
      "Epoch:3 / 10,lr: 0.02000 train loss:1.05813,train acc: 0.55614,valid loss:0.62939, valid acc:0.63180\n",
      "==============================>\n",
      "tp:292.0 | fp:199.0 | tn:262.0 | fn:200.0\n",
      "current lost : tensor(1.4482)\n",
      "current lost : tensor(0.7903)\n",
      "current lost : tensor(1.3094)\n",
      "current lost : tensor(0.9655)\n",
      "current lost : tensor(1.1459)\n",
      "current lost : tensor(0.7257)\n",
      "current lost : tensor(0.9552)\n",
      "current lost : tensor(1.7867)\n",
      "tp:139.0 | fp:99.0 | tn:0.0 | fn:1.0\n",
      "Epoch:4 / 10,lr: 0.02000 train loss:0.87266,train acc: 0.58132,valid loss:1.14086, valid acc:0.58159\n",
      "==============================>\n",
      "tp:309.0 | fp:228.0 | tn:233.0 | fn:183.0\n",
      "current lost : tensor(0.7346)\n",
      "current lost : tensor(0.6299)\n",
      "current lost : tensor(0.8684)\n",
      "current lost : tensor(0.7740)\n",
      "current lost : tensor(0.7034)\n",
      "current lost : tensor(0.6643)\n",
      "current lost : tensor(0.6198)\n",
      "current lost : tensor(0.7145)\n",
      "tp:21.0 | fp:2.0 | tn:97.0 | fn:119.0\n",
      "Epoch:5 / 10,lr: 0.02000 train loss:0.79796,train acc: 0.56873,valid loss:0.71360, valid acc:0.49372\n",
      "==============================>\n",
      "tp:296.0 | fp:214.0 | tn:247.0 | fn:196.0\n",
      "current lost : tensor(1.0320)\n",
      "current lost : tensor(0.9274)\n",
      "current lost : tensor(0.7712)\n",
      "current lost : tensor(0.6449)\n",
      "current lost : tensor(0.5519)\n",
      "current lost : tensor(0.7637)\n",
      "current lost : tensor(0.8261)\n",
      "current lost : tensor(1.2122)\n",
      "tp:140.0 | fp:99.0 | tn:0.0 | fn:0.0\n",
      "Epoch:6 / 10,lr: 0.02000 train loss:0.76639,train acc: 0.56978,valid loss:0.84118, valid acc:0.58577\n",
      "==============================>\n",
      "tp:303.0 | fp:220.0 | tn:241.0 | fn:189.0\n",
      "current lost : tensor(0.7650)\n",
      "current lost : tensor(0.7252)\n",
      "current lost : tensor(0.6135)\n",
      "current lost : tensor(0.7205)\n",
      "current lost : tensor(0.5895)\n",
      "current lost : tensor(0.7006)\n",
      "current lost : tensor(0.6737)\n",
      "current lost : tensor(0.8705)\n",
      "tp:76.0 | fp:39.0 | tn:60.0 | fn:64.0\n",
      "Epoch:7 / 10,lr: 0.01000 train loss:0.73058,train acc: 0.57083,valid loss:0.70732, valid acc:0.56904\n",
      "==============================>\n",
      "tp:348.0 | fp:194.0 | tn:267.0 | fn:144.0\n",
      "current lost : tensor(0.5692)\n",
      "current lost : tensor(0.5942)\n",
      "current lost : tensor(0.5954)\n",
      "current lost : tensor(0.6209)\n",
      "current lost : tensor(0.5673)\n",
      "current lost : tensor(0.5846)\n",
      "current lost : tensor(0.6102)\n",
      "current lost : tensor(0.6141)\n",
      "tp:109.0 | fp:39.0 | tn:60.0 | fn:31.0\n",
      "Epoch:8 / 10,lr: 0.01000 train loss:0.64140,train acc: 0.64533,valid loss:0.59451, valid acc:0.70711\n",
      "==============================>\n",
      "tp:339.0 | fp:210.0 | tn:251.0 | fn:153.0\n",
      "current lost : tensor(0.8746)\n",
      "current lost : tensor(0.7432)\n",
      "current lost : tensor(0.9067)\n",
      "current lost : tensor(0.7969)\n",
      "current lost : tensor(0.9053)\n",
      "current lost : tensor(0.6497)\n",
      "current lost : tensor(0.8907)\n",
      "current lost : tensor(0.7360)\n",
      "tp:11.0 | fp:2.0 | tn:97.0 | fn:129.0\n",
      "Epoch:9 / 10,lr: 0.01000 train loss:0.65006,train acc: 0.61910,valid loss:0.81289, valid acc:0.45188\n",
      "==============================>\n",
      "tp:344.0 | fp:188.0 | tn:273.0 | fn:148.0\n",
      "current lost : tensor(0.6028)\n",
      "current lost : tensor(0.4939)\n",
      "current lost : tensor(0.6633)\n",
      "current lost : tensor(0.6224)\n",
      "current lost : tensor(0.7217)\n",
      "current lost : tensor(0.7345)\n",
      "current lost : tensor(0.5303)\n",
      "current lost : tensor(0.6419)\n",
      "tp:75.0 | fp:21.0 | tn:78.0 | fn:65.0\n",
      "Epoch:10 / 10,lr: 0.01000 train loss:0.64094,train acc: 0.64743,valid loss:0.62634, valid acc:0.64017\n",
      "Fold 3\n",
      "++++++++ Training ++++++++\n",
      "==============================>\n",
      "tp:286.0 | fp:195.0 | tn:251.0 | fn:222.0\n",
      "current lost : tensor(3.9752)\n",
      "current lost : tensor(4.7878)\n",
      "current lost : tensor(5.6225)\n",
      "current lost : tensor(4.5437)\n",
      "current lost : tensor(6.0510)\n",
      "current lost : tensor(4.3186)\n",
      "current lost : tensor(5.5034)\n",
      "current lost : tensor(6.7571)\n",
      "tp:124.0 | fp:113.0 | tn:1.0 | fn:0.0\n",
      "Epoch:1 / 10,lr: 0.02000 train loss:5.43330,train acc: 0.56289,valid loss:5.19492, valid acc:0.52521\n",
      "==============================>\n",
      "tp:295.0 | fp:218.0 | tn:228.0 | fn:213.0\n",
      "current lost : tensor(0.8491)\n",
      "current lost : tensor(1.2157)\n",
      "current lost : tensor(1.8143)\n",
      "current lost : tensor(1.5261)\n",
      "current lost : tensor(1.0017)\n",
      "current lost : tensor(1.2662)\n",
      "current lost : tensor(1.6722)\n",
      "current lost : tensor(1.7939)\n",
      "tp:124.0 | fp:114.0 | tn:0.0 | fn:0.0\n",
      "Epoch:2 / 10,lr: 0.02000 train loss:2.64430,train acc: 0.54822,valid loss:1.39240, valid acc:0.52101\n",
      "==============================>\n",
      "tp:289.0 | fp:213.0 | tn:233.0 | fn:219.0\n",
      "current lost : tensor(0.7758)\n",
      "current lost : tensor(0.8424)\n",
      "current lost : tensor(0.5656)\n",
      "current lost : tensor(0.6319)\n",
      "current lost : tensor(0.9712)\n",
      "current lost : tensor(0.7593)\n",
      "current lost : tensor(0.9835)\n",
      "current lost : tensor(0.8696)\n",
      "tp:120.0 | fp:85.0 | tn:29.0 | fn:4.0\n",
      "Epoch:3 / 10,lr: 0.02000 train loss:1.43332,train acc: 0.54717,valid loss:0.79992, valid acc:0.62605\n",
      "==============================>\n",
      "tp:324.0 | fp:229.0 | tn:217.0 | fn:184.0\n",
      "current lost : tensor(0.6644)\n",
      "current lost : tensor(0.7848)\n",
      "current lost : tensor(0.6566)\n",
      "current lost : tensor(0.7840)\n",
      "current lost : tensor(0.5674)\n",
      "current lost : tensor(0.7593)\n",
      "current lost : tensor(0.7942)\n",
      "current lost : tensor(0.6829)\n",
      "tp:16.0 | fp:2.0 | tn:112.0 | fn:108.0\n",
      "Epoch:4 / 10,lr: 0.02000 train loss:0.89628,train acc: 0.56709,valid loss:0.71168, valid acc:0.53782\n",
      "==============================>\n",
      "tp:322.0 | fp:173.0 | tn:273.0 | fn:186.0\n",
      "current lost : tensor(0.5878)\n",
      "current lost : tensor(0.5996)\n",
      "current lost : tensor(0.7211)\n",
      "current lost : tensor(0.7089)\n",
      "current lost : tensor(0.6355)\n",
      "current lost : tensor(0.5193)\n",
      "current lost : tensor(0.4295)\n",
      "current lost : tensor(0.8109)\n",
      "tp:116.0 | fp:78.0 | tn:36.0 | fn:8.0\n",
      "Epoch:5 / 10,lr: 0.02000 train loss:0.76729,train acc: 0.62369,valid loss:0.62657, valid acc:0.63866\n",
      "==============================>\n",
      "tp:359.0 | fp:206.0 | tn:240.0 | fn:149.0\n",
      "current lost : tensor(0.8201)\n",
      "current lost : tensor(0.8723)\n",
      "current lost : tensor(0.8706)\n",
      "current lost : tensor(0.7444)\n",
      "current lost : tensor(0.9832)\n",
      "current lost : tensor(0.5963)\n",
      "current lost : tensor(0.8638)\n",
      "current lost : tensor(0.7324)\n",
      "tp:3.0 | fp:0.0 | tn:114.0 | fn:121.0\n",
      "Epoch:6 / 10,lr: 0.02000 train loss:0.67825,train acc: 0.62788,valid loss:0.81037, valid acc:0.49160\n",
      "==============================>\n",
      "tp:362.0 | fp:204.0 | tn:242.0 | fn:146.0\n",
      "current lost : tensor(0.6611)\n",
      "current lost : tensor(0.6573)\n",
      "current lost : tensor(0.5250)\n",
      "current lost : tensor(0.5607)\n",
      "current lost : tensor(0.5476)\n",
      "current lost : tensor(0.6073)\n",
      "current lost : tensor(0.6198)\n",
      "current lost : tensor(0.4930)\n",
      "tp:72.0 | fp:20.0 | tn:94.0 | fn:52.0\n",
      "Epoch:7 / 10,lr: 0.01000 train loss:0.64731,train acc: 0.63312,valid loss:0.58398, valid acc:0.69748\n",
      "==============================>\n",
      "tp:384.0 | fp:216.0 | tn:230.0 | fn:124.0\n",
      "current lost : tensor(0.6778)\n",
      "current lost : tensor(0.6474)\n",
      "current lost : tensor(0.6516)\n",
      "current lost : tensor(0.6145)\n",
      "current lost : tensor(0.5932)\n",
      "current lost : tensor(0.6196)\n",
      "current lost : tensor(0.6016)\n",
      "current lost : tensor(0.6613)\n",
      "tp:52.0 | fp:8.0 | tn:106.0 | fn:72.0\n",
      "Epoch:8 / 10,lr: 0.01000 train loss:0.63315,train acc: 0.64361,valid loss:0.63339, valid acc:0.66387\n",
      "==============================>\n",
      "tp:384.0 | fp:189.0 | tn:257.0 | fn:124.0\n",
      "current lost : tensor(0.6345)\n",
      "current lost : tensor(0.5634)\n",
      "current lost : tensor(0.6335)\n",
      "current lost : tensor(0.5865)\n",
      "current lost : tensor(0.5812)\n",
      "current lost : tensor(0.5333)\n",
      "current lost : tensor(0.5203)\n",
      "current lost : tensor(0.6610)\n",
      "tp:109.0 | fp:61.0 | tn:53.0 | fn:15.0\n",
      "Epoch:9 / 10,lr: 0.01000 train loss:0.62322,train acc: 0.67191,valid loss:0.58921, valid acc:0.68067\n",
      "==============================>\n",
      "tp:342.0 | fp:180.0 | tn:266.0 | fn:166.0\n",
      "current lost : tensor(0.6822)\n",
      "current lost : tensor(0.6815)\n",
      "current lost : tensor(0.6766)\n",
      "current lost : tensor(0.6567)\n",
      "current lost : tensor(0.6738)\n",
      "current lost : tensor(0.6764)\n",
      "current lost : tensor(0.6511)\n",
      "current lost : tensor(0.6969)\n",
      "tp:89.0 | fp:30.0 | tn:84.0 | fn:35.0\n",
      "Epoch:10 / 10,lr: 0.01000 train loss:0.62896,train acc: 0.63732,valid loss:0.67439, valid acc:0.72689\n",
      "Fold 4\n",
      "++++++++ Training ++++++++\n",
      "==============================>\n",
      "tp:255.0 | fp:192.0 | tn:257.0 | fn:250.0\n",
      "current lost : tensor(4.3957)\n",
      "current lost : tensor(6.7474)\n",
      "current lost : tensor(5.4767)\n",
      "current lost : tensor(3.5619)\n",
      "current lost : tensor(4.6764)\n",
      "current lost : tensor(4.1540)\n",
      "current lost : tensor(3.6400)\n",
      "current lost : tensor(5.3792)\n",
      "tp:127.0 | fp:109.0 | tn:2.0 | fn:0.0\n",
      "Epoch:1 / 10,lr: 0.02000 train loss:4.85961,train acc: 0.53669,valid loss:4.75391, valid acc:0.54202\n",
      "==============================>\n",
      "tp:305.0 | fp:211.0 | tn:238.0 | fn:200.0\n",
      "current lost : tensor(0.6049)\n",
      "current lost : tensor(0.7027)\n",
      "current lost : tensor(0.6488)\n",
      "current lost : tensor(0.6423)\n",
      "current lost : tensor(0.6135)\n",
      "current lost : tensor(0.5320)\n",
      "current lost : tensor(0.5053)\n",
      "current lost : tensor(0.4610)\n",
      "tp:78.0 | fp:35.0 | tn:76.0 | fn:49.0\n",
      "Epoch:2 / 10,lr: 0.02000 train loss:1.75894,train acc: 0.56918,valid loss:0.58882, valid acc:0.64706\n",
      "==============================>\n",
      "tp:288.0 | fp:232.0 | tn:217.0 | fn:217.0\n",
      "current lost : tensor(0.6479)\n",
      "current lost : tensor(0.5564)\n",
      "current lost : tensor(0.6839)\n",
      "current lost : tensor(0.4752)\n",
      "current lost : tensor(0.6781)\n",
      "current lost : tensor(0.4890)\n",
      "current lost : tensor(0.6849)\n",
      "current lost : tensor(0.6213)\n",
      "tp:68.0 | fp:22.0 | tn:89.0 | fn:59.0\n",
      "Epoch:3 / 10,lr: 0.02000 train loss:1.08331,train acc: 0.52935,valid loss:0.60460, valid acc:0.65966\n",
      "==============================>\n",
      "tp:330.0 | fp:218.0 | tn:231.0 | fn:175.0\n",
      "current lost : tensor(1.1526)\n",
      "current lost : tensor(1.0711)\n",
      "current lost : tensor(1.2517)\n",
      "current lost : tensor(1.0151)\n",
      "current lost : tensor(0.8579)\n",
      "current lost : tensor(1.0067)\n",
      "current lost : tensor(1.0653)\n",
      "current lost : tensor(0.9034)\n",
      "tp:127.0 | fp:111.0 | tn:0.0 | fn:0.0\n",
      "Epoch:4 / 10,lr: 0.02000 train loss:0.82112,train acc: 0.58805,valid loss:1.04048, valid acc:0.53361\n",
      "==============================>\n",
      "tp:328.0 | fp:221.0 | tn:228.0 | fn:177.0\n",
      "current lost : tensor(0.5567)\n",
      "current lost : tensor(0.5324)\n",
      "current lost : tensor(0.5245)\n",
      "current lost : tensor(0.6494)\n",
      "current lost : tensor(0.6552)\n",
      "current lost : tensor(0.6142)\n",
      "current lost : tensor(0.6231)\n",
      "current lost : tensor(0.4499)\n",
      "tp:109.0 | fp:53.0 | tn:58.0 | fn:18.0\n",
      "Epoch:5 / 10,lr: 0.02000 train loss:0.77407,train acc: 0.58281,valid loss:0.57568, valid acc:0.70168\n",
      "==============================>\n",
      "tp:333.0 | fp:223.0 | tn:226.0 | fn:172.0\n",
      "current lost : tensor(0.4990)\n",
      "current lost : tensor(0.6013)\n",
      "current lost : tensor(0.7093)\n",
      "current lost : tensor(0.6098)\n",
      "current lost : tensor(0.5490)\n",
      "current lost : tensor(0.5294)\n",
      "current lost : tensor(0.5718)\n",
      "current lost : tensor(0.5055)\n",
      "tp:113.0 | fp:52.0 | tn:59.0 | fn:14.0\n",
      "Epoch:6 / 10,lr: 0.02000 train loss:0.70087,train acc: 0.58595,valid loss:0.57189, valid acc:0.72269\n",
      "==============================>\n",
      "tp:317.0 | fp:222.0 | tn:227.0 | fn:188.0\n",
      "current lost : tensor(0.7387)\n",
      "current lost : tensor(0.8047)\n",
      "current lost : tensor(0.5426)\n",
      "current lost : tensor(0.5995)\n",
      "current lost : tensor(0.6372)\n",
      "current lost : tensor(0.9430)\n",
      "current lost : tensor(0.6723)\n",
      "current lost : tensor(0.8987)\n",
      "tp:127.0 | fp:107.0 | tn:4.0 | fn:0.0\n",
      "Epoch:7 / 10,lr: 0.01000 train loss:0.73952,train acc: 0.57023,valid loss:0.72960, valid acc:0.55042\n",
      "==============================>\n",
      "tp:320.0 | fp:224.0 | tn:225.0 | fn:185.0\n",
      "current lost : tensor(0.6394)\n",
      "current lost : tensor(0.6292)\n",
      "current lost : tensor(0.6889)\n",
      "current lost : tensor(0.6499)\n",
      "current lost : tensor(0.6637)\n",
      "current lost : tensor(0.6764)\n",
      "current lost : tensor(0.5762)\n",
      "current lost : tensor(0.7036)\n",
      "tp:35.0 | fp:11.0 | tn:100.0 | fn:92.0\n",
      "Epoch:8 / 10,lr: 0.01000 train loss:0.67988,train acc: 0.57128,valid loss:0.65341, valid acc:0.56723\n",
      "==============================>\n",
      "tp:332.0 | fp:201.0 | tn:248.0 | fn:173.0\n",
      "current lost : tensor(0.6609)\n",
      "current lost : tensor(0.5834)\n",
      "current lost : tensor(0.6326)\n",
      "current lost : tensor(0.6095)\n",
      "current lost : tensor(0.5745)\n",
      "current lost : tensor(0.6232)\n",
      "current lost : tensor(0.6298)\n",
      "current lost : tensor(0.6250)\n",
      "tp:99.0 | fp:40.0 | tn:71.0 | fn:28.0\n",
      "Epoch:9 / 10,lr: 0.01000 train loss:0.65325,train acc: 0.60797,valid loss:0.61737, valid acc:0.71429\n",
      "==============================>\n",
      "tp:353.0 | fp:228.0 | tn:221.0 | fn:152.0\n",
      "current lost : tensor(0.6556)\n",
      "current lost : tensor(0.6190)\n",
      "current lost : tensor(0.6133)\n",
      "current lost : tensor(0.6658)\n",
      "current lost : tensor(0.6421)\n",
      "current lost : tensor(0.6082)\n",
      "current lost : tensor(0.6582)\n",
      "current lost : tensor(0.6184)\n",
      "tp:73.0 | fp:26.0 | tn:85.0 | fn:54.0\n",
      "Epoch:10 / 10,lr: 0.01000 train loss:0.66363,train acc: 0.60168,valid loss:0.63508, valid acc:0.66387\n",
      "Fold 5\n",
      "++++++++ Training ++++++++\n",
      "==============================>\n",
      "tp:280.0 | fp:212.0 | tn:235.0 | fn:227.0\n",
      "current lost : tensor(1.1990)\n",
      "current lost : tensor(1.0714)\n",
      "current lost : tensor(2.0405)\n",
      "current lost : tensor(1.3305)\n",
      "current lost : tensor(1.1973)\n",
      "current lost : tensor(1.1930)\n",
      "current lost : tensor(0.8850)\n",
      "current lost : tensor(1.1868)\n",
      "tp:72.0 | fp:25.0 | tn:88.0 | fn:53.0\n",
      "Epoch:1 / 10,lr: 0.02000 train loss:5.47420,train acc: 0.53983,valid loss:1.26295, valid acc:0.67227\n",
      "==============================>\n",
      "tp:311.0 | fp:210.0 | tn:237.0 | fn:196.0\n",
      "current lost : tensor(0.8011)\n",
      "current lost : tensor(0.7131)\n",
      "current lost : tensor(0.7722)\n",
      "current lost : tensor(0.7923)\n",
      "current lost : tensor(0.9340)\n",
      "current lost : tensor(0.7732)\n",
      "current lost : tensor(0.9730)\n",
      "current lost : tensor(0.5651)\n",
      "tp:107.0 | fp:72.0 | tn:41.0 | fn:18.0\n",
      "Epoch:2 / 10,lr: 0.02000 train loss:1.77925,train acc: 0.57442,valid loss:0.79051, valid acc:0.62185\n",
      "==============================>\n",
      "tp:298.0 | fp:202.0 | tn:245.0 | fn:209.0\n",
      "current lost : tensor(0.7502)\n",
      "current lost : tensor(0.8483)\n",
      "current lost : tensor(0.7505)\n",
      "current lost : tensor(0.6041)\n",
      "current lost : tensor(0.5876)\n",
      "current lost : tensor(0.7577)\n",
      "current lost : tensor(0.7326)\n",
      "current lost : tensor(0.6792)\n",
      "tp:21.0 | fp:3.0 | tn:110.0 | fn:104.0\n",
      "Epoch:3 / 10,lr: 0.02000 train loss:1.50667,train acc: 0.56918,valid loss:0.71377, valid acc:0.55042\n",
      "==============================>\n",
      "tp:305.0 | fp:222.0 | tn:225.0 | fn:202.0\n",
      "current lost : tensor(0.8277)\n",
      "current lost : tensor(0.8980)\n",
      "current lost : tensor(0.7686)\n",
      "current lost : tensor(0.8125)\n",
      "current lost : tensor(0.8030)\n",
      "current lost : tensor(0.6468)\n",
      "current lost : tensor(0.7949)\n",
      "current lost : tensor(0.4729)\n",
      "tp:32.0 | fp:7.0 | tn:106.0 | fn:93.0\n",
      "Epoch:4 / 10,lr: 0.02000 train loss:0.92685,train acc: 0.55556,valid loss:0.75306, valid acc:0.57983\n",
      "==============================>\n",
      "tp:317.0 | fp:217.0 | tn:230.0 | fn:190.0\n",
      "current lost : tensor(0.6799)\n",
      "current lost : tensor(0.8388)\n",
      "current lost : tensor(0.6198)\n",
      "current lost : tensor(0.6024)\n",
      "current lost : tensor(0.6129)\n",
      "current lost : tensor(0.5386)\n",
      "current lost : tensor(0.6554)\n",
      "current lost : tensor(0.8091)\n",
      "tp:121.0 | fp:91.0 | tn:22.0 | fn:4.0\n",
      "Epoch:5 / 10,lr: 0.02000 train loss:0.80724,train acc: 0.57338,valid loss:0.66962, valid acc:0.60084\n",
      "==============================>\n",
      "tp:337.0 | fp:205.0 | tn:242.0 | fn:170.0\n",
      "current lost : tensor(0.5892)\n",
      "current lost : tensor(0.6315)\n",
      "current lost : tensor(0.8460)\n",
      "current lost : tensor(0.6332)\n",
      "current lost : tensor(0.5987)\n",
      "current lost : tensor(0.6439)\n",
      "current lost : tensor(0.6689)\n",
      "current lost : tensor(0.8018)\n",
      "tp:73.0 | fp:28.0 | tn:85.0 | fn:52.0\n",
      "Epoch:6 / 10,lr: 0.02000 train loss:0.73864,train acc: 0.60692,valid loss:0.67666, valid acc:0.66387\n",
      "==============================>\n",
      "tp:333.0 | fp:201.0 | tn:246.0 | fn:174.0\n",
      "current lost : tensor(0.7633)\n",
      "current lost : tensor(0.6432)\n",
      "current lost : tensor(0.7260)\n",
      "current lost : tensor(0.6068)\n",
      "current lost : tensor(0.5927)\n",
      "current lost : tensor(0.6486)\n",
      "current lost : tensor(0.6873)\n",
      "current lost : tensor(0.6565)\n",
      "tp:119.0 | fp:91.0 | tn:22.0 | fn:6.0\n",
      "Epoch:7 / 10,lr: 0.01000 train loss:0.69792,train acc: 0.60692,valid loss:0.66554, valid acc:0.59244\n",
      "==============================>\n",
      "tp:349.0 | fp:206.0 | tn:241.0 | fn:158.0\n",
      "current lost : tensor(0.9106)\n",
      "current lost : tensor(0.6597)\n",
      "current lost : tensor(0.6771)\n",
      "current lost : tensor(0.7254)\n",
      "current lost : tensor(0.7066)\n",
      "current lost : tensor(0.7596)\n",
      "current lost : tensor(0.7435)\n",
      "current lost : tensor(0.6865)\n",
      "tp:7.0 | fp:0.0 | tn:113.0 | fn:118.0\n",
      "Epoch:8 / 10,lr: 0.01000 train loss:0.67071,train acc: 0.61845,valid loss:0.73360, valid acc:0.50420\n",
      "==============================>\n",
      "tp:330.0 | fp:200.0 | tn:247.0 | fn:177.0\n",
      "current lost : tensor(0.6917)\n",
      "current lost : tensor(0.6571)\n",
      "current lost : tensor(0.6441)\n",
      "current lost : tensor(0.6902)\n",
      "current lost : tensor(0.6744)\n",
      "current lost : tensor(0.6655)\n",
      "current lost : tensor(0.6261)\n",
      "current lost : tensor(0.5606)\n",
      "tp:121.0 | fp:97.0 | tn:16.0 | fn:4.0\n",
      "Epoch:9 / 10,lr: 0.01000 train loss:0.67184,train acc: 0.60482,valid loss:0.65121, valid acc:0.57563\n",
      "==============================>\n",
      "tp:365.0 | fp:205.0 | tn:242.0 | fn:142.0\n",
      "current lost : tensor(0.6039)\n",
      "current lost : tensor(0.5957)\n",
      "current lost : tensor(0.6234)\n",
      "current lost : tensor(0.6351)\n",
      "current lost : tensor(0.6184)\n",
      "current lost : tensor(0.6298)\n",
      "current lost : tensor(0.6542)\n",
      "current lost : tensor(0.6735)\n",
      "tp:85.0 | fp:33.0 | tn:80.0 | fn:40.0\n",
      "Epoch:10 / 10,lr: 0.01000 train loss:0.65317,train acc: 0.63627,valid loss:0.62924, valid acc:0.69328\n",
      "\n",
      "\n",
      "Performance of 5 fold cross validation\n",
      "average train results\n",
      "loss      : 1.2895467\n",
      "acc       : 0.5867011\n",
      "precision : 0.60394025\n",
      "recall    : 0.63640785\n",
      "f1 score  : 0.6192953\n",
      "average val results\n",
      "loss      : 0.95837796\n",
      "acc       : 0.60112685\n",
      "precision : 0.69760704\n",
      "recall    : 0.63643265\n",
      "f1 score  : 0.57015836\n",
      "tp:43.0 | fp:9.0 | tn:60.0 | fn:38.0\n",
      "\n",
      "\n",
      "Test Accuracy: tensor(0.6867) Test Precision: tensor(0.8269) Test Recall: tensor(0.5309) Test F1: tensor(0.6466)\n"
     ]
    }
   ],
   "source": [
    "hist2 = train_model(params4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
